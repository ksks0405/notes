---
title: "Carter & Kohn (1994)"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: show
---


# Settings

The chapter contains some chunks that conduct document settings.

-   This chunk **removes all the variables** in the environment.

```{r SET housekeeping, class.source='fold-hide'}
rm(list = ls(all = TRUE))
```

-   This chunk **sets** `knitr` **options**.

```{r SET knit options, class.source='fold-hide'}
knitr::opts_chunk$set(
  block.title = TRUE,
  fig.align = "center",
  results = "hold",
  fig.show = "hold",
  message = FALSE,
  warning = FALSE,
  class.source = 'fold-hide'
)
knitr::opts_hooks$set(label = function(options) {
  options$before = paste0('<div>Chunk: ', options$label, '</div>')
  return(options)
})
```

-   This chunk **loads the required packages** and **my functions**.

```{r SET packages, class.source='fold-hide'}
pkgList <- c(
  "tidyverse" #must-have
)
easypackages::libraries(pkgList)
rm(pkgList)
```

-   This chunk sets **the seed**.

```{r SET seeds, class.source='fold-hide'}
set.seed(1111)
```

# セットアップ（Carter & Kohn の記法）

Carter & Kohn (1994) の基本モデルは

$$
\begin{alignedat}{2}
y(t) &= h(t)^\prime x(t) + e(t), &\quad \text{(1-1)}\\
x(t+1) &= F(t+1)x(t) + u(t+1). &\quad\text{(1-2)}
\end{alignedat}
$$

- 観測：スカラー $y(t)$  
- 状態：$m \times 1$ の $x(t)$  
- indicator：$K(t)$（誤差の mixture 成分など）  
- $K = \{K(1), \ldots,K(n)\}$ を与えると、誤差が条件付きガウスになり、(1-1)(1-2) はガウス状態空間


# §2-2 の要点：後ろ向き分解（Lemma 2-1）

Lemma 2-1 は（$K,\theta$ を固定した条件付きの下で）

$$
p(X \mid Y^n)=p(x(n) \mid Y^n)\\  \prod_{t=1}^{n-1} p(x(t) \mid Y^t, x(t+1)),
$$

という分解を与えます。

実装は

1. **前向き**：Kalman filter で $x(t \mid t)$ と $S(t \mid t)$ を全時点保存  
2. **後ろ向き**：
   - $x(n) \sim p(x(n) \mid Y^n)=N(x(n \mid n),S(n \mid n))$
   - $t=n-1, \ldots,1$ で $x(t) \sim p(x(t) \mid Y^t,x(t+1))$

です。

# 数値例

## データ生成（fictitious data）

```{r}
# Model: x_{t+1} = 0.7 x_t + u_t,  y_t = x_t + e_t
# Var settings (model): Var(e_t)=1, Var(u_t)=1 or 4 depending on K

n <- 4
h <- 0.7

# Observation error variance (model setting)
R <- 1
C <- 4
K <- c(0, 1, 0, 1)
R_t <- ifelse(K == 0, R, C * R)

# State innovation variances (model setting)
tau <- 1

# Fixed shocks
x1 <- 0
u  <- c(1,  2, -1)                 # u1,u2,u3 (fixed)
e  <- c(0.5, -0.5, 1, -1)          # e1..e4 (fixed)

# Build state and observation
x <- numeric(n)
x[1] <- x1
for(t in 1:(n-1)){
  x[t+1] <- h * x[t] + u[t]
}
y <- x + e

truth <- data.frame(t = 1:n, K = K, x = x, y = y, e = e, 
                    u = c(u, NA), Var_e = rep(tau, n), Var_u = R_t)
```

```{r}
truth
```

## Kを条件づけて状態列を一括生成

Carter & Kohn (1994) Chap.2 ベース：スカラー状態空間 + mixture 観測誤差

$$
\begin{alignedat}{2}
y_t &= h\,x_t + e_t, && \qquad t=1,\ldots,n,\\
x_{t+1} &= F\,x_t + u_t,&& \qquad t=1,\ldots,n-1.
\end{alignedat}
$$
である。ここで $y_t$ はスカラー観測、$x_t$ はスカラー状態、$h$ は既知（あるいはパラメータ）のスカラー係数、$e_t$ は観測誤差、 $F$ は既知（あるいはパラメータ）のスカラー係数、$u_t$ は状態方程式のイノベーションである。

状態イノベーションは同分散（homoskedastic）な分散$\tau>0$を用いて下記のように表す。
$$
u_t \sim N(0,\tau),\qquad t=1,\ldots,n-1,
$$

観測誤差は二成分の混合正規（mixture normal）であり、潜在 indicator $K_t\in\{0,1\}$、 基準分散 $R>0$ 、 スケール $C>1$ により以下のように表す。
$$
e_t \mid K_t=0 \sim N(0,R),\qquad
e_t \mid K_t=1 \sim N(0,CR), \qquad \Pr(K_t=0)=p,\qquad \Pr(K_t=1)=1-p.
$$

$\{K_t\}$ は（少なくとも事前には）時点間独立であるとする。

初期状態には便宜的に
$$
x_1 \sim N(a_1, s_1)
$$
を仮定する。ここで $a_1$ は初期平均、$s_1$ は初期分散である。

さらに、条件付きでガウス状態空間になるように、$K=(K_1,\ldots,K_n)$ を与えたもとで
$\{e_t\}$ と $\{u_t\}$ は互いに独立、かつそれぞれ時点間独立であると仮定する。

% ---- 条件付き観測分散（given K） ----
$K_t$ を与えると観測誤差分散は時点ごとに
$$
R_t := \mathrm{Var}(e_t\mid K_t)=
\begin{cases}
R & (K_t=0),\\
CR & (K_t=1),
\end{cases}
$$
と確定し、モデルは $(h,F,R_t,\tau)$ を持つ線形ガウス状態空間モデルとして扱える。

予測量（forward filteringで使う記号）

forward filtering では、予測分布
$$
x_t \mid y^{t-1} \sim N(a_t, s_t)
$$
を定義する。ここで $y^{t-1}=(y_1,\ldots,y_{t-1})$ である。
このとき予測された観測分布は
$$
y_t \mid y^{t-1} \sim N(b_t, v_t),
$$
ただし
$$
b_t := E(y_t\mid y^{t-1})=h\,a_t,\qquad
v_t := \mathrm{Var}(y_t\mid y^{t-1})=h^2 s_t + R_t
$$
である。


```{r FUN forward}
ck_forward_filter_scalar_mixture <- function(y, h, F, R, C, K, tau, a1, s1) {
  # --- checks ---
  y <- as.numeric(y)
  n <- length(y)
  if (length(K) != n) stop("K must have the same length as y (n).")
  if (any(!K %in% c(0, 1))) stop("K must be a 0/1 vector.")
  if (R <= 0) stop("R must be > 0 (variance).")
  if (tau   <= 0) stop("tau must be > 0 (variance).")
  if (C     <= 0) stop("C must be > 0.")
  if (s1    <= 0) stop("s1 must be > 0 (variance).")

  # conditional obs variances R_t = Var(e_t | K_t)
  R_t <- ifelse(K == 0, R, C * R)

  # prior moments BEFORE seeing y_t
  a <- numeric(n)  # a_t = E[x_t | y^{t-1}]
  s <- numeric(n)  # s_t = Var[x_t | y^{t-1}]
  b <- numeric(n)  # b_t = E[y_t | y^{t-1}]
  v <- numeric(n)  # v_t = Var[y_t | y^{t-1}]

  # posterior moments AFTER seeing y_t
  a_upd <- numeric(n)  # a_t^{+} = E[x_t | y^{t}]
  s_upd <- numeric(n)  # s_t^{+} = Var[x_t | y^{t}]

  # Kalman gain
  gain <- numeric(n)

  # initial predictive for t=1
  a[1] <- a1
  s[1] <- s1

  for (t in 1:n) {
    # predicted observation
    b[t] <- h * a[t]
    v[t] <- (h^2) * s[t] + R_t[t]

    # Kalman gain
    gain[t] <- (s[t] * h) / v[t]

    # update with y_t
    a_upd[t] <- a[t] + gain[t] * (y[t] - b[t])
    s_upd[t] <- s[t] - (gain[t]^2) * v[t]   # equivalent to (1 - gain[t]*h)*s[t]

    # predict next state (if t < n)
    if (t < n) {
      a[t + 1] <- F * a_upd[t]
      s[t + 1] <- (F^2) * s_upd[t] + tau
    }
  }

  list(
    n = n, y = y, h = h, F = F, R = R, C = C, K = K, tau = tau, 
    a1 = a1, s1 = s1, R_t = R_t, a = a, s = s, b = b, v = v, 
    a_upd = a_upd, s_upd = s_upd, gain = gain
  )
}
```

$x_1$の事前予測は、
$$ 
\begin{aligned}
x_1 &\sim N(a_1, s_1), \,\text{where}\\
a_1 &= E[x_1|Y^0]\\
s_1 &= E[(x_1-\hat{x}_1)^2|Y^0]
\end{aligned}
$$

$y_1$の事前予測は、$y_t = hx_t + e_t,\ (e_t \sim \text{mixture})$より、

$$
\begin{aligned}
y_1 &\sim N(b_1, v_1), \, \text{where}\\
b_1 &= hE[x_1|Y^0] = ha_1,\\
v_1 &= E[(y_1-\hat{y}_1)^2|Y^0] = E[\{h(x-\hat{x}_1)\}^2|Y^0] =h^2s_1+R_1
\end{aligned}
$$

実現値との回帰

$$
\boxed{x_1-\hat{x}_1=L_1(y_1-\hat{y}_1) + \eta_1 }
$$
の両辺に$(y_1-\hat{y}_1)$をかけ期待値を取ると、

$$ 
\begin{aligned}
\mathbb{E}[\underbrace{ (x_1-\hat{x}_1)(y_1-\hat{y}_1) }_{h(x_1-\hat{x}_1)(x_1-\hat{x}_1+e_t)}]&=L_1\mathbb{E}[(y_1-\hat{y}_1)^2]\\
\Rightarrow hs_1 &= L_1 (h^2s_1+R_1)
\end{aligned}
$$
$$
\therefore L_1 =  \frac{hs_1}{h^2s_1+R_1}
$$

数値例に当てはめると、

$$
L_1 = \begin{cases}
\dfrac{0.7 \times 1}{0.7^2\times 1+1} = \dfrac{0.7}{1.49} \fallingdotseq 0.470 \quad (\text{low volatility})\\[1em]
\dfrac{0.7 \times 1}{0.7^2\times 1+4} = \dfrac{0.7}{4.49} \fallingdotseq 0.156 \quad (\text{high volatility})
\end{cases}
$$

$$
a_1^+ := \underbrace{ \hat{x}_1 }_{a_1}+L_1\underbrace{(y_1-\hat{y}_1)}_{y_1-ha_1} = \begin{cases}
0 + 0.235 \times (0.5-0) = 0.235 \quad (\text{low volatility})\\[1em]
0 + 0.235 \times (0.5-0) = 0.078 \quad (\text{high volatility})
\end{cases}
$$

```{r}
fwd1 <- ck_forward_filter_scalar_mixture(
  y = y, h = h, F = 1, R = R, C = C, K = c(0, 0, 0, 0), a1 = 0, s1 = 1, tau = tau)
fwd2 <- ck_forward_filter_scalar_mixture(
  y = y, h = h, F = 1, R = R, C = C, K = c(1, 1, 1, 1), a1 = 0, s1 = 1, tau = tau)
0+(h)/((h)^2+R)*(y[1]-0)
0+(h)/((h)^2+R*C)*(y[1]-0)
```

```{r}
# --- 1) プロット用データを作る（prior / posterior） ---
mk_df <- function(fwd, case_label) {
  tibble(
    t = 1:fwd$n,
    case = case_label,
    prior = fwd$a,          # a_t = E[x_t | y^{t-1}]
    posterior = fwd$a_upd   # a_t^{+} = E[x_t | y^{t}]
  )
}

df12 <- bind_rows(
  mk_df(fwd1, "K=(0,0,0,0)"),
  mk_df(fwd2, "K=(1,1,1,1)")
) %>%
  pivot_longer(
    cols = c(prior, posterior),
    names_to = "series",
    values_to = "value"
  ) %>%
  mutate(
    series = factor(series, levels = c("prior", "posterior"))
  )

# --- 2) 観測値（×、線なし）は1本だけ ---
df_obs <- tibble(
  t = 1:length(y),
  y = as.numeric(y)
)

# --- 3) ggplot ---
ggplot() +
  # 観測値：×（線なし）
  geom_point(
    data = df_obs,
    aes(x = t, y = y),
    shape = 4, size = 3, stroke = 1
  ) +
  # prior / posterior：ケースで色分け、priorは点線、posteriorは実線
  geom_line(
    data = df12,
    aes(x = t, y = value, color = case, linetype = series),
    linewidth = 0.9
  ) +
  scale_linetype_manual(
    values = c(prior = "dashed", posterior = "solid")
  ) +
  labs(
    x = "t",
    y = "value",
    color = "Case (K)",
    linetype = "",
    title = "Prior (a_t) and Posterior (a_t^{+}) vs Observations"
  ) +
  theme_minimal()

```


```{r, eval = FALSE}
# backward draw（1回）
x_draw_1 <- ck_backward_sample_states_scalar(fwd)

data.frame(
  t = 1:n,
  x_true = x_true,
  x_draw = x_draw_1,
  y = y,
  K_true = K_true
)
```

## 6.3 複数ドローして（条件付き）事後平均の雰囲気を見る

```{r, eval = FALSE}
M <- 5000
Xdraw <- matrix(NA_real_, nrow = M, ncol = n)

for (j in 1:M) {
  Xdraw[j, ] <- ck_backward_sample_states_scalar(fwd)
}

post_mean <- colMeans(Xdraw)
post_sd <- apply(Xdraw, 2, sd)

data.frame(
  t = 1:n,
  x_true = x_true,
  post_mean = post_mean,
  post_sd = post_sd,
  y = y,
  K_true = K_true
)
```

# 7. 注意（Gibbs 全体にするには）

このRmdは「$K$ を与えた条件付きで $p(X \mid Y^n,K, \theta)$ から状態列を引く」部分だけを実装しています。  
Carter & Kohn の Gibbs 全体や Kim et al. のSV推計では、別途

- $p(K \mid Y^n,X, \theta)$ から $K$ を更新するステップ（離散）
- さらにパラメータ $\theta$ を更新するステップ

が加わります。
