---
title: "Carter & Kohn (1994)"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: show
---


# Settings

The chapter contains some chunks that conduct document settings.

-   This chunk **removes all the variables** in the environment.

```{r SET housekeeping, class.source='fold-hide'}
rm(list = ls(all = TRUE))
```

-   This chunk **sets** `knitr` **options**.

```{r SET knit options, class.source='fold-hide'}
knitr::opts_chunk$set(
  block.title = TRUE,
  fig.align = "center",
  results = "hold",
  fig.show = "hold",
  message = FALSE,
  warning = FALSE,
  class.source = 'fold-hide'
)
knitr::opts_hooks$set(label = function(options) {
  options$before = paste0('<div>Chunk: ', options$label, '</div>')
  return(options)
})
```

-   This chunk **loads the required packages** and **my functions**.

```{r SET packages, class.source='fold-hide'}
pkgList <- c(
  "tidyverse" #must-have
)
easypackages::libraries(pkgList)
rm(pkgList)
```

-   This chunk sets **the seed**.

```{r SET seeds, class.source='fold-hide'}
set.seed(1111)
```

# セットアップ（Carter & Kohn の記法）

Carter & Kohn (1994) の基本モデルは

$$
y(t) = h(t)^\prime x(t) + e(t), \quad \text{(1-1)}
$$

$$
x(t+1) = F(t+1)x(t) + u(t+1). \tag{1-2}
$$

- 観測：スカラー $y(t)$  
- 状態：$m \times 1$ の $x(t)$  
- indicator：$K(t)$（誤差の mixture 成分など）  
- $K = \{K(1), \ldots,K(n)\}$ を与えると、誤差が条件付きガウスになり、(1-1)(1-2) はガウス状態空間

本Rmdの目的は、Kim et al. で使う「状態列の同時生成」部分（Kalman filter + backward sampling）を、**そのまま実装できる粒度**で示すことです。

---

# §2-2 の要点：後ろ向き分解（Lemma 2-1）

Lemma 2-1 は（$K,\theta$ を固定した条件付きの下で）

$$
p(X \mid Y^n)=p(x(n) \mid Y^n)\\  \prod_{t=1}^{n-1} p(x(t) \mid Y^t, x(t+1)),
$$

という分解を与えます。

実装は

1. **前向き**：Kalman filter で $x(t \mid t)$ と $S(t \mid t)$ を全時点保存  
2. **後ろ向き**：
   - $x(n) \sim p(x(n) \mid Y^n)=N(x(n \mid n),S(n \mid n))$
   - $t=n-1, \ldots,1$ で $x(t) \sim p(x(t) \mid Y^t,x(t+1))$

です。

# 最小モデル（スカラー状態）と mixture normal 観測誤差

ここでは最小限として $m=1$ とし、

$$
y(t)=x(t)+e(t), \quad (h(t)=1)
$$

$$
x(t+1)=\phi x(t)+u(t+1),\quad (F(t+1)=\phi)
$$

$$
u(t) \sim N(0,r^2).
$$

観測誤差はユーザー指定の 2成分 mixture normal：

- 確率 $p_1$ で $e(t) \sim N(0, \sigma^2)$
- 確率 $1-p_1$ で $e(t) \sim N(0,C \sigma^2)$（$C>1$）

indicator を
- $K(t)=0$ なら $\mathrm{Var}(e(t))= \sigma^2$
- $K(t)=1$ なら $\mathrm{Var}(e(t))=C \sigma^2$

とします。

このとき $K$ を条件づけると、観測分散は時点ごとに
$$
R(t)=
\begin{cases}
 \sigma^2 & (K(t)=0)\\
C \sigma^2 & (K(t)=1)
\end{cases}
$$
に切り替わるだけで、Kalman filter / backward sampling は同じ形です。

---

# 4. 実装用の数式（スカラー版）

## 4.1 前向き（Kalman filter）

- 予測（$t\ge 2$）：
$$
a_t =  \phi m_{t-1},\quad
P_t =  \phi^2 C_{t-1}+r^2
$$

- 観測更新：
$$
Q_t = P_t + R(t),\quad
K_t = P_t/Q_t
$$
$$
m_t = a_t + K_t\{y(t)-a_t\},\quad
C_t = (1-K_t)P_t
$$

初期は $x(1) \sim N(m_1,C_1)$（proper）を置きます。

## 4.2 後ろ向き：$p(x(t) \mid Y^t,x(t+1))$

$$
 \Delta_t =  \phi^2 C_t + r^2,\quad
B_t =  \frac{C_t \phi}{ \Delta_t}
$$
$$
x(t) \mid (Y^t,x(t+1))  \sim N \Big(m_t + B_t\{x(t+1)- \phi m_t\},\ C_t - B_t^2 \Delta_t \Big).
$$

# 数値例（n = 4）

## データ生成（fictitious data）

```{r}
# Model: x_{t+1} = 0.7 x_t + u_t,  y_t = x_t + e_t
# Var settings (model): Var(e_t)=1, Var(u_t)=1 or 4 depending on K

n <- 4
phi <- 0.7

# Observation error variance (model setting)
R <- 1

# State innovation variances (model setting)
q_small <- 1
q_big   <- 4

K <- c(0, 1, 0, 1)   # K4 is unused for n=4 (no x5), but kept for display
q_u <- c(q_small, q_big, q_small)  # for u1,u2,u3

# Fixed shocks
x1 <- 0
u  <- c(1,  2, -1)                 # u1,u2,u3 (fixed)
e  <- c(0.5, -0.5, 1, -1)          # e1..e4 (fixed)

# Build state and observation
x <- numeric(n)
x[1] <- x1
for(t in 1:(n-1)){
  x[t+1] <- phi * x[t] + u[t]
}
y <- x + e

truth <- data.frame(t = 1:n, K = K, x = x, y = y, e = e, 
                    u = c(u, NA), Var_e = rep(R, n), Var_u = c(q_u, NA))
```

```{r}
truth
```

## 6.2 K を条件づけて状態列を一括生成（1回）

初期分布（$x_1$）と予測分布（$y_1|x_1$）：

$$
\begin{alignedat}{3}
&x_1 \sim N(0,1) && \quad\Rightarrow\quad m_{1|0}=0,\;P_{1|0}=1. \\
&y_1 = x_1 + e_1,\; e_1\sim N(0,1) && \quad\Rightarrow\quad y_1|Y^0 \sim N(m_{1|0},\,P_{1|0}+1)=N(0,2).
\end{alignedat}
$$

観測：$y_1 = 0.5$ → フィルタリング（$y_1$）

$$
\begin{alignedat}{3}
& Q_1 = P_{1|0}+1 = 2, && \qquad G_1 = \frac{P_{1|0}}{Q_1}=\frac{1}{2}.\\
& m_{1|1} = m_{1|0}+G_1(y_1-m_{1|0})=\frac{1}{2}y_1, && \qquad P_{1|1}=(1-G_1)P_{1|0}=\frac{1}{2}.
\end{alignedat}
$$

**モデル再掲**

Carter & Kohn (1994) Chap.2 ベース：スカラー状態空間 + mixture 観測誤差

$$
\begin{alignedat}{2}
y_t &= h\,x_t + e_t, && \qquad t=1,\ldots,n,\\
x_{t+1} &= F\,x_t + u_t,&& \qquad t=1,\ldots,n-1.
\end{alignedat}
$$
である。ここで $y_t$ はスカラー観測、$x_t$ はスカラー状態、$h$ は既知（あるいはパラメータ）のスカラー係数、$e_t$ は観測誤差、 $F$ は既知（あるいはパラメータ）のスカラー係数、$u_t$ は状態方程式のイノベーションである。

状態イノベーションは同分散（homoskedastic）な分散$\tau>0$を用いて下記のように表す。
$$
u_t \sim N(0,\tau),\qquad t=1,\ldots,n-1,
$$

観測誤差は二成分の混合正規（mixture normal）であり、潜在 indicator $K_t\in\{0,1\}$、 基準分散 $\sigma>0$ 、 スケール $C>1$ により以下のように表す。
$$
e_t \mid K_t=0 \sim N(0,\sigma),\qquad
e_t \mid K_t=1 \sim N(0,C\sigma), \qquad \Pr(K_t=0)=p,\qquad \Pr(K_t=1)=1-p.

$$

$\{K_t\}$ は（少なくとも事前には）時点間独立であるとする。

初期状態には便宜的に
$$
x_1 \sim N(a_1, s_1)
$$
を仮定する。ここで $a_1$ は初期平均、$s_1$ は初期分散である。

さらに、条件付きでガウス状態空間になるように、$K=(K_1,\ldots,K_n)$ を与えたもとで
$\{e_t\}$ と $\{u_t\}$ は互いに独立、かつそれぞれ時点間独立であると仮定する。

% ---- 条件付き観測分散（given K） ----
$K_t$ を与えると観測誤差分散は時点ごとに
$$
R_t := \mathrm{Var}(e_t\mid K_t)=
\begin{cases}
\sigma & (K_t=0),\\
C\sigma & (K_t=1),
\end{cases}
$$
と確定し、モデルは $(h,F,R_t,\tau)$ を持つ線形ガウス状態空間モデルとして扱える。

予測量（forward filteringで使う記号）

forward filtering では、予測分布
$$
x_t \mid y^{t-1} \sim N(a_t, s_t)
$$
を定義する。ここで $y^{t-1}=(y_1,\ldots,y_{t-1})$ である。
このとき予測された観測分布は
$$
y_t \mid y^{t-1} \sim N(b_t, v_t),
$$
ただし
$$
b_t := E(y_t\mid y^{t-1})=h\,a_t,\qquad
v_t := \mathrm{Var}(y_t\mid y^{t-1})=h^2 s_t + R_t
$$
である。


```{r}
ck_forward_filter_scalar_mixture <- function(y, h, F, sigma, C, K, tau, a1, s1) {
  # --- checks ---
  y <- as.numeric(y)
  n <- length(y)
  if (length(K) != n) stop("K must have the same length as y (n).")
  if (any(!K %in% c(0, 1))) stop("K must be a 0/1 vector.")
  if (sigma <= 0) stop("sigma must be > 0 (variance).")
  if (tau   <= 0) stop("tau must be > 0 (variance).")
  if (C     <= 0) stop("C must be > 0.")
  if (s1    <= 0) stop("s1 must be > 0 (variance).")

  # conditional obs variances R_t = Var(e_t | K_t)
  R_t <- ifelse(K == 0, sigma, C * sigma)

  # allocate
  a <- numeric(n)  # a_t = E[x_t | y^{t-1}]
  s <- numeric(n)  # s_t = Var[x_t | y^{t-1}]
  b <- numeric(n)  # b_t = E[y_t | y^{t-1}]
  v <- numeric(n)  # v_t = Var[y_t | y^{t-1}]

  # (needed for recursion) posterior moments after seeing y_t
  a_upd <- numeric(n)  # a_t^{+} = E[x_t | y^{t}]
  s_upd <- numeric(n)  # s_t^{+} = Var[x_t | y^{t}]

  # Kalman gain
  gain <- numeric(n)

  # initial predictive for t=1
  a[1] <- a1
  s[1] <- s1

  for (t in 1:n) {
    # predicted observation
    b[t] <- h * a[t]
    v[t] <- (h^2) * s[t] + R_t[t]

    # Kalman gain
    gain[t] <- (s[t] * h) / v[t]

    # update with y_t
    a_upd[t] <- a[t] + gain[t] * (y[t] - b[t])
    s_upd[t] <- s[t] - (gain[t]^2) * v[t]   # equivalent to (1 - gain[t]*h)*s[t]

    # predict next state (if t < n)
    if (t < n) {
      a[t + 1] <- F * a_upd[t]
      s[t + 1] <- (F^2) * s_upd[t] + tau
    }
  }

  list(
    n = n, y = y, h = h, F = F, sigma = sigma, C = C, K = K, tau = tau, 
    a1 = a1, s1 = s1, R_t = R_t, a = a, s = s, b = b, v = v, 
    a_upd = a_upd, s_upd = s_upd, gain = gain
  )
}
```


```{r, eval = FALSE}
# forward
fwd <- ck_forward_filter_scalar_mixture(
  y = y,
  phi = phi,
  sigma2 = sigma2,
  r2 = r2,
  C = C_big,
  K = K_true,
  m1 = 0,
  C1 = 1
)
tibble(t = c(1:4),
       y = y,
       x = x, )

fwd1 <- ck_forward_filter_scalar_mixture(
  y = y, h = phi, F = 1, sigma = 1, C = 2, K = c(0, 0, 0, 0), a1 = 0, s1 = 1, tau = 1)
)
fwd2 <- ck_forward_filter_scalar_mixture(
  y = y, h = phi, F = 1, sigma = 1, C = 1, K = K, a1 = 0, s1 = 1, tau = 1)
)
fwd3 <- ck_forward_filter_scalar_mixture(
  y = y, h = phi, F = 1, sigma = 1, C = 2, K = K, a1 = 0, s1 = 1, tau = 1)
)
fwd1$a_upd
fwd2$a_upd
fwd3$a_upd
```




```{r, eval = FALSE}
# backward draw（1回）
x_draw_1 <- ck_backward_sample_states_scalar(fwd)

data.frame(
  t = 1:n,
  x_true = x_true,
  x_draw = x_draw_1,
  y = y,
  K_true = K_true
)
```

---

## 6.3 複数ドローして（条件付き）事後平均の雰囲気を見る

```{r, eval = FALSE}
M <- 5000
Xdraw <- matrix(NA_real_, nrow = M, ncol = n)

for (j in 1:M) {
  Xdraw[j, ] <- ck_backward_sample_states_scalar(fwd)
}

post_mean <- colMeans(Xdraw)
post_sd <- apply(Xdraw, 2, sd)

data.frame(
  t = 1:n,
  x_true = x_true,
  post_mean = post_mean,
  post_sd = post_sd,
  y = y,
  K_true = K_true
)
```

---

# 7. 注意（Gibbs 全体にするには）

このRmdは「$K$ を与えた条件付きで $p(X \mid Y^n,K, \theta)$ から状態列を引く」部分だけを実装しています。  
Carter & Kohn の Gibbs 全体や Kim et al. のSV推計では、別途

- $p(K \mid Y^n,X, \theta)$ から $K$ を更新するステップ（離散）
- さらにパラメータ $\theta$ を更新するステップ

が加わります。
