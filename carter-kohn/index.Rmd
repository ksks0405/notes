---
title: "Carter & Kohn (1994) §2-2: Kalman filter + backward sampling（mixture normal 観測誤差）最小実装"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: show
---

# 1. セットアップ（Carter & Kohn の記法）

Carter & Kohn (1994) の基本モデルは

$$
y(t) = h(t)^\prime x(t) + e(t), \quad \text{(1-1)}
$$

$$
x(t+1) = F(t+1)x(t) + u(t+1). \tag{1-2}
$$

- 観測：スカラー $y(t)$  
- 状態：$m \times 1$ の $x(t)$  
- indicator：$K(t)$（誤差の mixture 成分など）  
- $K = \{K(1), \ldots,K(n)\}$ を与えると、誤差が条件付きガウスになり、(1-1)(1-2) はガウス状態空間

本Rmdの目的は、Kim et al. で使う「状態列の同時生成」部分（Kalman filter + backward sampling）を、**そのまま実装できる粒度**で示すことです。

---

# 2. §2-2 の要点：後ろ向き分解（Lemma 2-1）

Lemma 2-1 は（$K,\theta$ を固定した条件付きの下で）

$$
p(X \mid Y^n)=p(x(n) \mid Y^n)\\  \prod_{t=1}^{n-1} p(x(t) \mid Y^t, x(t+1)),
$$

という分解を与えます。

実装は

1. **前向き**：Kalman filter で $x(t \mid t)$ と $S(t \mid t)$ を全時点保存  
2. **後ろ向き**：
   - $x(n) \sim p(x(n) \mid Y^n)=N(x(n \mid n),S(n \mid n))$
   - $t=n-1, \ldots,1$ で $x(t) \sim p(x(t) \mid Y^t,x(t+1))$

です。

---

# 3. 最小モデル（スカラー状態）と mixture normal 観測誤差

ここでは最小限として $m=1$ とし、

$$
y(t)=x(t)+e(t), \quad (h(t)=1)
$$

$$
x(t+1)=\phi x(t)+u(t+1),\quad (F(t+1)=\phi)
$$

$$
u(t) \sim N(0,r^2).
$$

観測誤差はユーザー指定の 2成分 mixture normal：

- 確率 $p_1$ で $e(t) \sim N(0, \sigma^2)$
- 確率 $1-p_1$ で $e(t) \sim N(0,C \sigma^2)$（$C>1$）

indicator を
- $K(t)=0$ なら $\mathrm{Var}(e(t))= \sigma^2$
- $K(t)=1$ なら $\mathrm{Var}(e(t))=C \sigma^2$

とします。

このとき $K$ を条件づけると、観測分散は時点ごとに
$$
R(t)=
\begin{cases}
 \sigma^2 & (K(t)=0)\\
C \sigma^2 & (K(t)=1)
\end{cases}
$$
に切り替わるだけで、Kalman filter / backward sampling は同じ形です。

---

# 4. 実装用の数式（スカラー版）

## 4.1 前向き（Kalman filter）

- 予測（$t\ge 2$）：
$$
a_t =  \phi m_{t-1},\quad
P_t =  \phi^2 C_{t-1}+r^2
$$

- 観測更新：
$$
Q_t = P_t + R(t),\quad
K_t = P_t/Q_t
$$
$$
m_t = a_t + K_t\{y(t)-a_t\},\quad
C_t = (1-K_t)P_t
$$

初期は $x(1) \sim N(m_1,C_1)$（proper）を置きます。

## 4.2 後ろ向き：$p(x(t) \mid Y^t,x(t+1))$

$$
 \Delta_t =  \phi^2 C_t + r^2,\quad
B_t =  \frac{C_t \phi}{ \Delta_t}
$$
$$
x(t) \mid (Y^t,x(t+1))  \sim N \Big(m_t + B_t\{x(t+1)- \phi m_t\},\ C_t - B_t^2 \Delta_t \Big).
$$

---

# 5. R 実装（前向き関数・後ろ向き関数を分離）

## 5.1 前向き：Kalman filter（$K(t)$ による $R(t)$ の切替込み）

```{r}
ck_forward_filter_scalar_mixture <- function(y, phi, sigma2, r2, C, K,
                                             m1 = 0, C1 = 1) {
  # y: length n
  # K: length n, K(t) in {0,1}
  n <- length(y)
  stopifnot(length(K) == n)

  # 観測分散 R(t)
  R_t <- ifelse(K == 0, sigma2, C * sigma2)

  # 保存
  a  <- numeric(n)  # a_t = x(t|t-1)
  P  <- numeric(n)  # P_t = S(t|t-1)
  m  <- numeric(n)  # m_t = x(t|t)
  Cc <- numeric(n)  # C_t = S(t|t)
  Q  <- numeric(n)  # Q_t = P_t + R_t
  Kg <- numeric(n)  # Kalman gain

  for (t in 1:n) {
    if (t == 1) {
      a[t] <- m1
      P[t] <- C1
    } else {
      a[t] <- phi * m[t - 1]
      P[t] <- phi^2 * Cc[t - 1] + r2
    }

    Q[t]  <- P[t] + R_t[t]
    Kg[t] <- P[t] / Q[t]
    m[t]  <- a[t] + Kg[t] * (y[t] - a[t])
    Cc[t] <- (1 - Kg[t]) * P[t]
  }

  list(
    n = n,
    y = y,
    phi = phi,
    sigma2 = sigma2,
    r2 = r2,
    C = C,
    K = K,
    R_t = R_t,
    a = a, P = P, m = m, Cc = Cc, Q = Q, Kg = Kg,
    m1 = m1, C1 = C1
  )
}
```

## 5.2 後ろ向き：状態列を一括生成（Lemma 2-1 の分解をそのまま）

```{r}
ck_backward_sample_states_scalar <- function(fwd) {
  n   <- fwd$n
  phi <- fwd$phi
  r2  <- fwd$r2

  m  <- fwd$m
  Cc <- fwd$Cc

  x_draw <- numeric(n)

  # 1) x(n) ~ p(x(n) | Y^n) = N(m_n, C_n)
  x_draw[n] <- rnorm(1, mean = m[n], sd = sqrt(Cc[n]))

  # 2) for t = n-1,...,1: x(t) ~ p(x(t) | Y^t, x(t+1))
  if (n >= 2) {
    for (t in (n - 1):1) {
      Delta <- phi^2 * Cc[t] + r2
      B <- (Cc[t] * phi) / Delta

      mean_t <- m[t] + B * (x_draw[t + 1] - phi * m[t])
      var_t  <- Cc[t] - B^2 * Delta

      x_draw[t] <- rnorm(1, mean = mean_t, sd = sqrt(var_t))
    }
  }

  x_draw
}
```

---

# 6. 簡単な数値例（n = 4）— 真の e(t), u(t), state を記録

## 6.1 データ生成（fictitious data）

```{r}
# Fixed (non-random) dataset
# Model: x_{t+1} = 0.7 x_t + u_t,  y_t = x_t + e_t
# Var settings (model): Var(e_t)=1, Var(u_t)=1 or 4 depending on K

n <- 4
phi <- 0.7

# Observation error variance (model setting)
R <- 1

# State innovation variances (model setting)
q_small <- 1
q_big   <- 4

# Pattern A: K = (0,1,0,1)
# Interpret K1,K2,K3 as for u1,u2,u3 => Var(u) = (1,4,1)
# ----------------------------
K <- c(0, 1, 0, 1)   # K4 is unused for n=4 (no x5), but kept for display
q_u <- c(q_small, q_big, q_small)  # for u1,u2,u3

# Fixed shocks
x1 <- 0
u  <- c(1,  2, -1)                 # u1,u2,u3 (fixed)
e  <- c(0.5, -0.5, 1, -1)          # e1..e4 (fixed)

# Build state and observation
x <- numeric(n)
x[1] <- x1
for(t in 1:(n-1)){
  x[t+1] <- phi * x[t] + u[t]
}
y <- x + e

truth <- data.frame(
  t = 1:n,
  K = K,
  x = x,
  y = y,
  e = e,
  u = c(u, NA),          # align length to n for printing
  Var_e = rep(R, n),
  Var_u = c(q_u, NA)     # Var(u4) unused
)
truth
```

---

## 6.2 K を条件づけて状態列を一括生成（1回）

初期分布（$x_1$）と予測分布（$y_1|x_1$）：

$$
\begin{alignedat}{3}
&x_1 \sim N(0,1) && \quad\Rightarrow\quad m_{1|0}=0,\;P_{1|0}=1. \\
&y_1 = x_1 + e_1,\; e_1\sim N(0,1) && \quad\Rightarrow\quad y_1|Y^0 \sim N(m_{1|0},\,P_{1|0}+1)=N(0,2).
\end{alignedat}
$$

観測：$y_1 = 0.5$ → フィルタリング（$y_

$$
\begin{alignedat}{3}
& Q_1 = P_{1|0}+1 = 2, && \qquad G_1 = \frac{P_{1|0}}{Q_1}=\frac{1}{2}.\\
& m_{1|1} = m_{1|0}+G_1(y_1-m_{1|0})=\frac{1}{2}y_1, && \qquad P_{1|1}=(1-G_1)P_{1|0}=\frac{1}{2}.
\end{alignedat}
$$

```{r}
# forward
fwd <- ck_forward_filter_scalar_mixture(
  y = y,
  phi = phi,
  sigma2 = sigma2,
  r2 = r2,
  C = C_big,
  K = K_true,
  m1 = 0,
  C1 = 1
)
```




```{r}
# backward draw（1回）
x_draw_1 <- ck_backward_sample_states_scalar(fwd)

data.frame(
  t = 1:n,
  x_true = x_true,
  x_draw = x_draw_1,
  y = y,
  K_true = K_true
)
```

---

## 6.3 複数ドローして（条件付き）事後平均の雰囲気を見る

```{r}
M <- 5000
Xdraw <- matrix(NA_real_, nrow = M, ncol = n)

for (j in 1:M) {
  Xdraw[j, ] <- ck_backward_sample_states_scalar(fwd)
}

post_mean <- colMeans(Xdraw)
post_sd <- apply(Xdraw, 2, sd)

data.frame(
  t = 1:n,
  x_true = x_true,
  post_mean = post_mean,
  post_sd = post_sd,
  y = y,
  K_true = K_true
)
```

---

# 7. 注意（Gibbs 全体にするには）

このRmdは「$K$ を与えた条件付きで $p(X \mid Y^n,K, \theta)$ から状態列を引く」部分だけを実装しています。  
Carter & Kohn の Gibbs 全体や Kim et al. のSV推計では、別途

- $p(K \mid Y^n,X, \theta)$ から $K$ を更新するステップ（離散）
- さらにパラメータ $\theta$ を更新するステップ

が加わります。
