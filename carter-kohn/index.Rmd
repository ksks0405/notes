---
title: "Carter & Kohn (1994)"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: show
---


# Settings

The chapter contains some chunks that conduct document settings.

-   This chunk **removes all the variables** in the environment.

```{r SET housekeeping, class.source='fold-hide'}
rm(list = ls(all = TRUE))
```

-   This chunk **sets** `knitr` **options**.

```{r SET knit options, class.source='fold-hide'}
knitr::opts_chunk$set(
  block.title = TRUE,
  fig.align = "center",
  results = "hold",
  fig.show = "hold",
  message = FALSE,
  warning = FALSE,
  class.source = 'fold-hide'
)
knitr::opts_hooks$set(label = function(options) {
  options$before = paste0('<div>Chunk: ', options$label, '</div>')
  return(options)
})
```

-   This chunk **loads the required packages** and **my functions**.

```{r SET packages, class.source='fold-hide'}
pkgList <- c(
  "tidyverse" #must-have
)
easypackages::libraries(pkgList)
rm(pkgList)
```

-   This chunk sets **the seed**.

```{r SET seeds, class.source='fold-hide'}
set.seed(1111)
```

# セットアップ（Carter & Kohn の記法）

Carter & Kohn (1994) の基本モデルは

$$
\begin{alignedat}{2}
y(t) &= h(t)^\prime x(t) + e(t), &\quad \text{(1-1)}\\
x(t+1) &= F(t+1)x(t) + u(t+1). &\quad\text{(1-2)}
\end{alignedat}
$$

- 観測：スカラー $y(t)$  
- 状態：$m \times 1$ の $x(t)$  
- indicator：$K(t)$（誤差の mixture 成分など）  
- $K = \{K(1), \ldots,K(n)\}$ を与えると、誤差が条件付きガウスになり、(1-1)(1-2) はガウス状態空間


# §2-2 の要点：後ろ向き分解（Lemma 2-1）

Lemma 2-1 は（$K,\theta$ を固定した条件付きの下で）

$$
p(X \mid Y^n)=p(x(n) \mid Y^n)\\  \prod_{t=1}^{n-1} p(x(t) \mid Y^t, x(t+1)),
$$

という分解を与えます。

実装は

1. **前向き**：Kalman filter で $x(t \mid t)$ と $S(t \mid t)$ を全時点保存  
2. **後ろ向き**：
   - $x(n) \sim p(x(n) \mid Y^n)=N(x(n \mid n),S(n \mid n))$
   - $t=n-1, \ldots,1$ で $x(t) \sim p(x(t) \mid Y^t,x(t+1))$

です。

# FFBS数値例

## モデル

Carter & Kohn (1994) Chap.2をベースに、観測と状態がともにスカラーで、かつ観測係数と状態係数がともに定数（i.e. $F_t = F, \, h_t = h$）のモデルを考える：

$$
\begin{alignedat}{2}
y_t &= h\,x_t + e_t, && \qquad t=1,\ldots,n,\\
x_{t+1} &= F\,x_t + u_t,&& \qquad t=1,\ldots,n-1,
\end{alignedat}
$$
- $\{y_t\}_{t=1}^n$：スカラー観測、

- $\{x_t\}_{t=1}^n$：スカラー状態、

- $h$：観測方程式のスカラー係数、$e_t$：観測誤差、

- $F$：状態方程式のスカラー係数、$u_t$：状態のイノベーション

状態イノベーションは同分散な分散$\tau>0$を用いて下記のように表す。
$$
u_t \sim N(0,\tau),\qquad t=1,\ldots,n-1,
$$

観測誤差は**二成分の混合正規（mixture normal）**とし、基準分散 $R>0$ 、 スケール $C>1$、潜在indicator$K_t\in\{0,1\}$により以下のように表す。
$$
e_t \mid K_t=0 \sim N(0,R),\qquad
e_t \mid K_t=1 \sim N(0,CR), \qquad \Pr(K_t=0)=p,\qquad \Pr(K_t=1)=1-p.
$$

初期状態には便宜的に
$$
x_1 \sim N(a_1, s_1)
$$
を仮定する。ここで $a_1$ は初期平均、$s_1$ は初期分散である。

さらに、条件付きでガウス状態空間になるように、$K=(K_1,\ldots,K_n)$ を与えたもとで
$\{e_t\}$ と $\{u_t\}$ は互いに独立、かつそれぞれ時点間独立であると仮定する。

$K_t$ を与えると観測誤差分散は時点ごとに
$$
R_t := \mathrm{Var}(e_t\mid K_t)=
\begin{cases}
R & (K_t=0),\\
CR & (K_t=1),
\end{cases}
$$
と確定し、モデルは $(h,F,R_t,\tau)$ を持つ線形ガウス状態空間モデルとして扱える。

## データ生成（fictitious data）

以下では、真のモデルを$h=1$、$F=1$、$R = 1$、$C = 4$、$\{K_t\}_{t=1}^4 = \{0, 1, 0, 1\}$としてデータを生成する。ただし、状態・観測ノイズは、数値例を簡潔にするため手動で設定する。

```{r IMP parameters}
# Model: x_{t+1} = 0.7 x_t + u_t,  y_t = x_t + e_t
# Var settings (model): Var(e_t)=1, Var(u_t)=1 or 4 depending on K
n <- 4
h <- 0.7
R <- 1
C <- 4
K <- c(0, 1, 0, 1)
R_t <- ifelse(K == 0, R, C * R)
tau <- 1
```


```{r IMP dataset}
# Fixed shocks
x1 <- 0
u  <- c(1,  2, -1)                 # u1,u2,u3 (fixed)
e  <- c(0.5, -0.5, 1, -1)          # e1..e4 (fixed)

# Build state and observation
x <- numeric(n)
x[1] <- x1
for(t in 1:(n-1)){
  x[t+1] <- h * x[t] + u[t]
}
y <- x + e

truth <- data.frame(t = 1:n, K = K, x = x, y = y, e = e, 
                    u = c(u, NA), Var_e = rep(tau, n), Var_u = R_t)
```

```{r EXP dataset}
truth
```

### Forward filtering

forward filtering では、**1期前までの観測に基づく状態の予測分布**

$$
x_t \mid y^{t-1} \sim N(a_t, s_t)
$$

を定義する。ここで $y^{t-1}=(y_1,\ldots,y_{t-1})$ である。このとき、**観測の予測分布**は、
$$
y_t \mid y^{t-1} \sim N(b_t, v_t),
$$

$$
b_t := E(y_t\mid y^{t-1})=h\,a_t,\qquad
v_t := \mathrm{Var}(y_t\mid y^{t-1})=h^2 s_t + R_t
$$
である。

データにあたはめると、$x_1$の予測分布は、
$$ 
\begin{aligned}
x_1|y^0 &\sim N(a_1, s_1), \,\text{where}\\
a_1 &= E[x_1|Y^0]\\
s_1 &= E[(x_1-a_1)^2|Y^0]
\end{aligned}
$$

$y_1$の予測分布は、$y_t = hx_t + e_t,\ (e_t \sim \text{mixture})$より、

$$
\begin{aligned}
y_1|y^0 &\sim N(b_1, v_1), \, \text{where}\\
b_1 &= hE[x_1|Y^0] = ha_1,\\
v_1 &= E[(y_1-b_1)^2|Y^0] = E[\{h(x_1-a_1)\}^2|Y^0] =h^2s_1+R_1
\end{aligned}
$$

実現値との回帰

$$
\boxed{\underbrace{ x_1-E[x_1|y^0] }_{x_1 \text{の予測分布からの乖離をアップデート}}=L_1\underbrace{ (y_1-E[y_1|y^0]) }_{y_1 \text{の実現値からの乖離を基に}} + \eta_1 }
$$
の両辺に$(y_1-b_1)$をかけ期待値を取ると、

$$ 
\begin{alignedat}{2}
&E\Big[\underbrace{ (x_1-\underbrace{ E[x_1|y^0] }_{=a_1})(y_1-\underbrace{ E[y_1|y^0] }_{=b_1 = h a_1}) }_{h(x_1-a_1)(x_1-a_1+e_1)}\Big] &&= L_1E\Big[(y_1-E[y_1|y^0])^2\Big]\\
\Rightarrow & \qquad h \,\text{var}[x_1|y^0] &&= L_1  \, \text{var}[y_1|y^0]\\
\Rightarrow & \qquad hs_1 &&= L_1 (h^2s_1+R_1)
\end{alignedat}
$$
$$
\therefore L_1 =  \frac{hs_1}{h^2s_1+R_1}
$$

#### Forward sampling 実装 

数値例に当てはめると、

$$
L_1 = \begin{cases}
\dfrac{0.7 \times 1}{0.7^2\times 1+1} = \dfrac{0.7}{1.49} \fallingdotseq 0.470 \quad (\text{low volatility})\\[1em]
\dfrac{0.7 \times 1}{0.7^2\times 1+4} = \dfrac{0.7}{4.49} \fallingdotseq 0.156 \quad (\text{high volatility})
\end{cases}
$$

$$
a_1^+ := \underbrace{ E[{x}_1|y^0] }_{a_1}+L_1\underbrace{(y_1-E[y_1|y^0])}_{y_1-ha_1} = \begin{cases}
0 + 0.470 \times (0.5-0) = 0.235 \quad (\text{low volatility})\\[1em]
0 + 0.156 \times (0.5-0) = 0.078 \quad (\text{high volatility})
\end{cases}
$$

以上のステップを基に、forward filtering関数`ck_forward_filter_scalar_mixture`を書く。

```{r FUN ff}
ck_forward_filter_scalar_mixture <- function(y, h, F, R, C, K, tau, a1, s1) {
  # --- checks ---
  y <- as.numeric(y)
  n <- length(y)
  if (length(K) != n) stop("K must have the same length as y (n).")
  if (any(!K %in% c(0, 1))) stop("K must be a 0/1 vector.")
  if (R <= 0) stop("R must be > 0 (variance).")
  if (tau   <= 0) stop("tau must be > 0 (variance).")
  if (C     <= 0) stop("C must be > 0.")
  if (s1    <= 0) stop("s1 must be > 0 (variance).")

  # conditional obs variances R_t = Var(e_t | K_t)
  R_t <- ifelse(K == 0, R, C * R)

  # prior moments BEFORE seeing y_t
  a <- numeric(n)  # a_t = E[x_t | y^{t-1}]
  s <- numeric(n)  # s_t = Var[x_t | y^{t-1}]
  b <- numeric(n)  # b_t = E[y_t | y^{t-1}]
  v <- numeric(n)  # v_t = Var[y_t | y^{t-1}]

  # posterior moments AFTER seeing y_t
  a_upd <- numeric(n)  # a_t^{+} = E[x_t | y^{t}]
  s_upd <- numeric(n)  # s_t^{+} = Var[x_t | y^{t}]

  # Kalman gain
  gain <- numeric(n)

  # initial predictive for t=1
  a[1] <- a1
  s[1] <- s1

  for (t in 1:n) {
    # predicted observation
    b[t] <- h * a[t]
    v[t] <- (h^2) * s[t] + R_t[t]

    # Kalman gain
    gain[t] <- (s[t] * h) / v[t]

    # update with y_t
    a_upd[t] <- a[t] + gain[t] * (y[t] - b[t])
    s_upd[t] <- s[t] - (gain[t]^2) * v[t]   # equivalent to (1 - gain[t]*h)*s[t]

    # predict next state (if t < n)
    if (t < n) {
      a[t + 1] <- F * a_upd[t]
      s[t + 1] <- (F^2) * s_upd[t] + tau
    }
  }

  list(
    n = n, y = y, h = h, F = F, R = R, C = C, K = K, tau = tau, 
    a1 = a1, s1 = s1, R_t = R_t, a = a, s = s, b = b, v = v, 
    a_upd = a_upd, s_upd = s_upd, gain = gain
  )
}
```

手計算とも一致する。

```{r REG ff function vs handroll, class.source='fold-show'}
fwd <- ck_forward_filter_scalar_mixture(
  y = y, h = h, F = 1, R = R, C = C, K = K, a1 = 0, s1 = 1, tau = tau)
fwd1 <- ck_forward_filter_scalar_mixture(
  y = y, h = h, F = 1, R = R, C = C, K = c(0, 0, 0, 0), a1 = 0, s1 = 1, tau = tau)
fwd2 <- ck_forward_filter_scalar_mixture(
  y = y, h = h, F = 1, R = R, C = C, K = c(1, 1, 1, 1), a1 = 0, s1 = 1, tau = tau)
0+(h)/((h)^2+R)*(y[1]-0)
0+(h)/((h)^2+R*C)*(y[1]-0)
```

以下は、$h, F, C, R$の値を真の値で既知とし、2つの異なる$\{K\}$を仮定してフィルタリング分布の平均値を示している。$t = 1, 2, 3$を見ると、priorより大きい観測値を受けて、観測誤差のノイズを小さく見積もっている$\{K\}=\{0,0,0,0\}$ケースでより大きくposteriorを修正している。

```{r EXP ff plot}
# --- 1) プロット用データを作る（prior / posterior） ---
mk_df <- function(fwd, case_label) {
  tibble(
    t = 1:fwd$n,
    case = case_label,
    prior = fwd$a,          # a_t = E[x_t | y^{t-1}]
    posterior = fwd$a_upd   # a_t^{+} = E[x_t | y^{t}]
  )
}

df12 <- bind_rows(
  mk_df(fwd1, "K=(0,0,0,0)"),
  mk_df(fwd2, "K=(1,1,1,1)")
) %>%
  pivot_longer(
    cols = c(prior, posterior),
    names_to = "series",
    values_to = "value"
  ) %>%
  mutate(
    series = factor(series, levels = c("prior", "posterior"))
  )

# --- 2) 観測値（×、線なし）は1本だけ ---
df_obs <- tibble(
  t = 1:length(y),
  y = as.numeric(y)
)

# --- 3) ggplot ---
ggplot() +
  # 観測値：×（線なし）
  geom_point(
    data = df_obs,
    aes(x = t, y = y),
    shape = 4, size = 3, stroke = 1
  ) +
  # prior / posterior：ケースで色分け、priorは点線、posteriorは実線
  geom_line(
    data = df12,
    aes(x = t, y = value, color = case, linetype = series),
    linewidth = 0.9
  ) +
  scale_linetype_manual(
    values = c(prior = "dashed", posterior = "solid")
  ) +
  labs(
    x = "t",
    y = "value",
    color = "Case (K)",
    linetype = "",
    title = "Prior (a_t) and Posterior (a_t^{+}) vs Observations"
  ) +
  theme_minimal()

```

### Backward smoothing / sampling

$x_T$の事後予測（$x_T|y^T \sim N(a_T^{+}, s_T^{+}) $）は、**forward filteringの最終ステップ**から直ちに得られる。

**Backward smoothing**では、$x_{T-1}$のフィルタリング分布（$x_{T-1}|y_{T-1}$）を、$x_T|y_{T-1}$と$x_T$の差（すなわち$T-1$期時点の予測分布からの乖離＝$T$時点の情報を反映）に回帰。これにより、**フィルタリング分布に逐次的に未来の情報を取り込んでいく**：

$$
\boxed{ \underbrace{x_{T-1}-E[x_{T-1}|y^{T-1}] }_{x_{T-1}\text{のフィルタ分布からの乖離をアップデート}} =  J_{t-1}\underbrace{(x_T- E[x_T|y^{T-1}]) }_{x_T\text{の予測分布からの乖離を基に}}+\xi_t}
$$

両辺に$(x_T-E[x_T|y^{T-1}])$をかけ期待値を取ると、
$$
\begin{aligned}
E\Big[\underbrace{ (x_{T-1}-\underbrace{ E[x_{T-1}|y^{T-1} }_{=a_{T-1}^+}])(x_T-\underbrace{ E[x_T|y^{T-1}] }_{Fa_{T-1}^+}) }_{F(x_{T-1}-a_{T-1}^+)(x_{T-1}-a_{T-1}^+ + u_{T-1})}\Big] &=  J_{t-1}E\Big[(x_T-E[x_T|y^{T-1}])^2\Big]\\
\Rightarrow F \,\text{var}[x_{T-1}|y^{T-1}] &=J_{t-1} \text{var}[x_T|y^{{T-1}}]\\
\Rightarrow J_{t-1}  &= \frac{F s_{T-1}^+}{F^2s_{T-1}^+ + \tau}\\
\end{aligned}
$$

$$
\therefore J_{t-1}  = \frac{F s_{T-1}^+}{F^2s_{T-1}^+ + \tau} 
$$

**注意点：SamplingとSmoothingの違い**

- Samplingは、$x_T$（サンプル値）を使う

- Smoothingは、$\tilde{a}_T = E[x_T|y^T]$を使う

#### Backward smoothing 実装 

```{r FUN backward smoothing}
ck_backward_smoother_scalar <- function(fwd) {
  # fwd: output of ck_forward_filter_scalar_mixture()
  n   <- fwd$n
  F   <- fwd$F
  tau <- fwd$tau

  a_filt <- fwd$a_upd   # a_t^+ = E[x_t | y^t]
  s_filt <- fwd$s_upd   # s_t^+ = Var[x_t | y^t]

  # One-step-ahead predictions from filtered moments:
  # a_{t+1|t} = F a_t^+
  # s_{t+1|t} = F^2 s_t^+ + tau
  a_pred_next <- rep(NA_real_, n)
  s_pred_next <- rep(NA_real_, n)
  for (t in 1:(n - 1)) {
    a_pred_next[t] <- F * a_filt[t]
    s_pred_next[t] <- (F^2) * s_filt[t] + tau
  }

  # RTS smoothing gain J_t
  J <- rep(NA_real_, n)
  for (t in 1:(n - 1)) {
    J[t] <- (s_filt[t] * F) / s_pred_next[t]
  }

  # Smoothed moments
  a_smooth <- rep(NA_real_, n)  # E[x_t | y^n]
  s_smooth <- rep(NA_real_, n)  # Var[x_t | y^n]

  # Initialize at terminal time: smoothing = filtering
  a_smooth[n] <- a_filt[n]
  s_smooth[n] <- s_filt[n]

  # Backward recursion
  if (n >= 2) {
    for (t in (n - 1):1) {
      a_smooth[t] <- a_filt[t] + J[t] * (a_smooth[t + 1] - a_pred_next[t])
      s_smooth[t] <- s_filt[t] + (J[t]^2) * (s_smooth[t + 1] - s_pred_next[t])
      s_smooth[t] <- max(s_smooth[t], 0)  # numerical safety
    }
  }

  list(
    n = n,
    a_smooth = a_smooth,
    s_smooth = s_smooth,
    J = J,
    a_filt = a_filt,
    s_filt = s_filt,
    a_pred_next = a_pred_next,
    s_pred_next = s_pred_next
  )
}

```

```{r}
sm1 <- ck_backward_smoother_scalar(fwd1)
sm2 <- ck_backward_smoother_scalar(fwd2)

# --- 4) backward smoother の点予測（E[x_t|y^T]）を df に ---
mk_sm_df <- function(sm, case_label) {
  tibble(
    t = 1:sm$n,
    case = case_label,
    smooth = sm$a_smooth   # E[x_t | y^T]
  )
}

df_sm <- bind_rows(
  mk_sm_df(sm1, "K=(0,0,0,0)"),
  mk_sm_df(sm2, "K=(1,1,1,1)")
)

# --- 5) 既存プロットに追加（caseで色は合わせ、線種だけ別にする） ---
ggplot() +
  # 観測値：×（線なし）
  geom_point(
    data = df_obs,
    aes(x = t, y = y),
    shape = 4, size = 3, stroke = 1
  ) +
  # prior / posterior：ケースで色分け、priorは点線、posteriorは実線
  geom_line(
    data = df12,
    aes(x = t, y = value, color = case, linetype = series),
    linewidth = 0.9
  ) +
  scale_linetype_manual(
    values = c(prior = "dashed", posterior = "solid", smooth = "dotdash")
  ) +
  # backward smoothing 点予測：dotdash（caseで色分けは同じ）
  geom_line(
    data = df_sm,
    aes(x = t, y = smooth, color = case, linetype = "smooth"),
    linewidth = 1.1
  ) +
  labs(
    x = "t",
    y = "value",
    color = "Case (K)",
    linetype = "",
    title = "Prior (a_t), Posterior (a_t^{+}), and Backward Smoother E[x_t|y^T] vs Observations"
  ) +
  theme_minimal()

```

#### Backward sampling 実装 

```{r FUN backward sampling}
ck_backward_sampler_scalar <- function(fwd) {
  # Draw x_{1:n} ~ p(x_{1:n} | y^n, K, theta) under linear-Gaussian (K fixed)
  n   <- fwd$n
  F   <- fwd$F
  tau <- fwd$tau

  a_filt <- fwd$a_upd
  s_filt <- fwd$s_upd

  # One-step-ahead prediction moments from filtered moments:
  a_pred_next <- rep(NA_real_, n)
  s_pred_next <- rep(NA_real_, n)
  for (t in 1:(n - 1)) {
    a_pred_next[t] <- F * a_filt[t]
    s_pred_next[t] <- (F^2) * s_filt[t] + tau
  }

  # Smoothing gain J_t = Cov(x_t,x_{t+1}|y^t) / Var(x_{t+1}|y^t)
  J <- rep(NA_real_, n)
  for (t in 1:(n - 1)) {
    J[t] <- (s_filt[t] * F) / s_pred_next[t]
  }

  x <- rep(NA_real_, n)

  # 1) Draw terminal state from filtering distribution
  x[n] <- rnorm(1, mean = a_filt[n], sd = sqrt(s_filt[n]))

  # 2) Draw backward: x_t | (y^t, x_{t+1})
  if (n >= 2) {
    for (t in (n - 1):1) {
      # Conditional mean: a_t^+ + J_t (x_{t+1} - a_{t+1|t})
      mean_t <- a_filt[t] + J[t] * (x[t + 1] - a_pred_next[t])

      # Conditional variance: s_t^+ - J_t^2 * Var(x_{t+1}|y^t)
      var_t  <- s_filt[t] - (J[t]^2) * s_pred_next[t]
      var_t  <- max(var_t, 0)  # numerical safety

      x[t] <- rnorm(1, mean = mean_t, sd = sqrt(var_t))
    }
  }

  list(
    x_draw = x,
    J = J,
    a_filt = a_filt,
    s_filt = s_filt,
    a_pred_next = a_pred_next,
    s_pred_next = s_pred_next
  )
}

```

```{r}
# 1) Backward smoothing (RTS): E[x_t | y^T] and +/- 1 sd band
sm <- ck_backward_smoother_scalar(fwd)

df_band <- tibble(
  t = 1:sm$n,
  mean = sm$a_smooth,
  sd   = sqrt(sm$s_smooth),
  lo   = mean - sd,
  hi   = mean + sd
)

# 2) Backward sampling: 10 sample paths
set.seed(123)  # optional (for reproducibility)
M <- 50
Xdraw <- replicate(M, ck_backward_sampler_scalar(fwd)$x_draw)  # n x M

df_draws <- as_tibble(Xdraw) %>%
  mutate(t = 1:sm$n) %>%
  pivot_longer(
    cols = -t,
    names_to = "draw",
    values_to = "x"
  ) %>%
  mutate(draw = factor(draw))

# 3) Plot: band + smoothed mean + 10 sampled trajectories
ggplot() +
  geom_ribbon(data = df_band, aes(x = t, ymin = lo, ymax = hi), alpha = 0.2) +
  geom_line(data = df_band, aes(x = t, y = mean), linewidth = 1.1) +
  geom_line(data = df_draws, aes(x = t, y = x, group = draw), linewidth = 0.5, alpha = 0.5) +
  labs(
    x = "t", y = "state", 
    title = "Backward smoothing: E[x_t | y^T] ± 1 SD, and 10 backward-sampling draws"
  ) +
  theme_minimal()


```

# 注意（Gibbs 全体にするには）

このRmdは「$K$ を与えた条件付きで $p(X \mid Y^n,K, \theta)$ から状態列を引く」部分だけを実装しています。  
Carter & Kohn の Gibbs 全体や Kim et al. のSV推計では、別途

- $p(K \mid Y^n,X, \theta)$ から $K$ を更新するステップ（離散）
- さらにパラメータ $\theta$ を更新するステップ

が加わります。


