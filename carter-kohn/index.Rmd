---
title: "Carter & Kohn (1994)"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: show
---


# Settings

The chapter contains some chunks that conduct document settings.

-   This chunk **removes all the variables** in the environment.

```{r SET housekeeping, class.source='fold-hide'}
rm(list = ls(all = TRUE))
```

-   This chunk **sets** `knitr` **options**.

```{r SET knit options, class.source='fold-hide'}
knitr::opts_chunk$set(
  block.title = TRUE,
  fig.align = "center",
  results = "hold",
  fig.show = "hold",
  message = FALSE,
  warning = FALSE,
  class.source = 'fold-hide'
)
knitr::opts_hooks$set(label = function(options) {
  options$before = paste0('<div>Chunk: ', options$label, '</div>')
  return(options)
})
```

-   This chunk **loads the required packages** and **my functions**.

```{r SET packages, class.source='fold-hide'}
pkgList <- c(
  "tidyverse" #must-have
)
easypackages::libraries(pkgList)
rm(pkgList)
```

-   This chunk sets **the seed**.

```{r SET seeds, class.source='fold-hide'}
set.seed(1111)
```

# セットアップ（Carter & Kohn の記法）

Carter & Kohn (1994) の基本モデルは

$$
y(t) = h(t)^\prime x(t) + e(t), \quad \text{(1-1)}
$$

$$
x(t+1) = F(t+1)x(t) + u(t+1). \tag{1-2}
$$

- 観測：スカラー $y(t)$  
- 状態：$m \times 1$ の $x(t)$  
- indicator：$K(t)$（誤差の mixture 成分など）  
- $K = \{K(1), \ldots,K(n)\}$ を与えると、誤差が条件付きガウスになり、(1-1)(1-2) はガウス状態空間

本Rmdの目的は、Kim et al. で使う「状態列の同時生成」部分（Kalman filter + backward sampling）を、**そのまま実装できる粒度**で示すことです。

---

# §2-2 の要点：後ろ向き分解（Lemma 2-1）

Lemma 2-1 は（$K,\theta$ を固定した条件付きの下で）

$$
p(X \mid Y^n)=p(x(n) \mid Y^n)\\  \prod_{t=1}^{n-1} p(x(t) \mid Y^t, x(t+1)),
$$

という分解を与えます。

実装は

1. **前向き**：Kalman filter で $x(t \mid t)$ と $S(t \mid t)$ を全時点保存  
2. **後ろ向き**：
   - $x(n) \sim p(x(n) \mid Y^n)=N(x(n \mid n),S(n \mid n))$
   - $t=n-1, \ldots,1$ で $x(t) \sim p(x(t) \mid Y^t,x(t+1))$

です。

# 数値例

## データ生成（fictitious data）

```{r}
# Model: x_{t+1} = 0.7 x_t + u_t,  y_t = x_t + e_t
# Var settings (model): Var(e_t)=1, Var(u_t)=1 or 4 depending on K

n <- 4
h <- 0.7

# Observation error variance (model setting)
R <- 1
C <- 4
K <- c(0, 1, 0, 1)
R_t <- ifelse(K == 0, R, C * R)

# State innovation variances (model setting)
tau <- 1

# Fixed shocks
x1 <- 0
u  <- c(1,  2, -1)                 # u1,u2,u3 (fixed)
e  <- c(0.5, -0.5, 1, -1)          # e1..e4 (fixed)

# Build state and observation
x <- numeric(n)
x[1] <- x1
for(t in 1:(n-1)){
  x[t+1] <- h * x[t] + u[t]
}
y <- x + e

truth <- data.frame(t = 1:n, K = K, x = x, y = y, e = e, 
                    u = c(u, NA), Var_e = rep(tau, n), Var_u = R_t)
```

```{r}
truth
```

## Kを条件づけて状態列を一括生成

Carter & Kohn (1994) Chap.2 ベース：スカラー状態空間 + mixture 観測誤差

$$
\begin{alignedat}{2}
y_t &= h\,x_t + e_t, && \qquad t=1,\ldots,n,\\
x_{t+1} &= F\,x_t + u_t,&& \qquad t=1,\ldots,n-1.
\end{alignedat}
$$
である。ここで $y_t$ はスカラー観測、$x_t$ はスカラー状態、$h$ は既知（あるいはパラメータ）のスカラー係数、$e_t$ は観測誤差、 $F$ は既知（あるいはパラメータ）のスカラー係数、$u_t$ は状態方程式のイノベーションである。

状態イノベーションは同分散（homoskedastic）な分散$\tau>0$を用いて下記のように表す。
$$
u_t \sim N(0,\tau),\qquad t=1,\ldots,n-1,
$$

観測誤差は二成分の混合正規（mixture normal）であり、潜在 indicator $K_t\in\{0,1\}$、 基準分散 $R>0$ 、 スケール $C>1$ により以下のように表す。
$$
e_t \mid K_t=0 \sim N(0,R),\qquad
e_t \mid K_t=1 \sim N(0,CR), \qquad \Pr(K_t=0)=p,\qquad \Pr(K_t=1)=1-p.
$$

$\{K_t\}$ は（少なくとも事前には）時点間独立であるとする。

初期状態には便宜的に
$$
x_1 \sim N(a_1, s_1)
$$
を仮定する。ここで $a_1$ は初期平均、$s_1$ は初期分散である。

さらに、条件付きでガウス状態空間になるように、$K=(K_1,\ldots,K_n)$ を与えたもとで
$\{e_t\}$ と $\{u_t\}$ は互いに独立、かつそれぞれ時点間独立であると仮定する。

% ---- 条件付き観測分散（given K） ----
$K_t$ を与えると観測誤差分散は時点ごとに
$$
R_t := \mathrm{Var}(e_t\mid K_t)=
\begin{cases}
R & (K_t=0),\\
CR & (K_t=1),
\end{cases}
$$
と確定し、モデルは $(h,F,R_t,\tau)$ を持つ線形ガウス状態空間モデルとして扱える。

予測量（forward filteringで使う記号）

forward filtering では、予測分布
$$
x_t \mid y^{t-1} \sim N(a_t, s_t)
$$
を定義する。ここで $y^{t-1}=(y_1,\ldots,y_{t-1})$ である。
このとき予測された観測分布は
$$
y_t \mid y^{t-1} \sim N(b_t, v_t),
$$
ただし
$$
b_t := E(y_t\mid y^{t-1})=h\,a_t,\qquad
v_t := \mathrm{Var}(y_t\mid y^{t-1})=h^2 s_t + R_t
$$
である。


```{r FUN forward}
ck_forward_filter_scalar_mixture <- function(y, h, F, R, C, K, tau, a1, s1) {
  # --- checks ---
  y <- as.numeric(y)
  n <- length(y)
  if (length(K) != n) stop("K must have the same length as y (n).")
  if (any(!K %in% c(0, 1))) stop("K must be a 0/1 vector.")
  if (R <= 0) stop("R must be > 0 (variance).")
  if (tau   <= 0) stop("tau must be > 0 (variance).")
  if (C     <= 0) stop("C must be > 0.")
  if (s1    <= 0) stop("s1 must be > 0 (variance).")

  # conditional obs variances R_t = Var(e_t | K_t)
  R_t <- ifelse(K == 0, R, C * R)

  # prior moments BEFORE seeing y_t
  a <- numeric(n)  # a_t = E[x_t | y^{t-1}]
  s <- numeric(n)  # s_t = Var[x_t | y^{t-1}]
  b <- numeric(n)  # b_t = E[y_t | y^{t-1}]
  v <- numeric(n)  # v_t = Var[y_t | y^{t-1}]

  # posterior moments AFTER seeing y_t
  a_upd <- numeric(n)  # a_t^{+} = E[x_t | y^{t}]
  s_upd <- numeric(n)  # s_t^{+} = Var[x_t | y^{t}]

  # Kalman gain
  gain <- numeric(n)

  # initial predictive for t=1
  a[1] <- a1
  s[1] <- s1

  for (t in 1:n) {
    # predicted observation
    b[t] <- h * a[t]
    v[t] <- (h^2) * s[t] + R_t[t]

    # Kalman gain
    gain[t] <- (s[t] * h) / v[t]

    # update with y_t
    a_upd[t] <- a[t] + gain[t] * (y[t] - b[t])
    s_upd[t] <- s[t] - (gain[t]^2) * v[t]   # equivalent to (1 - gain[t]*h)*s[t]

    # predict next state (if t < n)
    if (t < n) {
      a[t + 1] <- F * a_upd[t]
      s[t + 1] <- (F^2) * s_upd[t] + tau
    }
  }

  list(
    n = n, y = y, h = h, F = F, R = R, C = C, K = K, tau = tau, 
    a1 = a1, s1 = s1, R_t = R_t, a = a, s = s, b = b, v = v, 
    a_upd = a_upd, s_upd = s_upd, gain = gain
  )
}
```


```{r}
# forward
fwd1 <- ck_forward_filter_scalar_mixture(
  y = y, h = h, F = 1, R = R, C = C, K = c(0, 0, 0, 0), a1 = 0, s1 = 1, tau = tau)
fwd2 <- ck_forward_filter_scalar_mixture(
  y = y, h = h, F = 1, R = R, C = C, K = c(1, 1, 1, 1), a1 = 0, s1 = 1, tau = tau)
fwd1$a_upd
fwd2$a_upd
(h)/((h)^2+R)
(h)/((h)^2+R*C)
fwd1$gain[1]
fwd2$gain[1]
```


```{r, eval = FALSE}
# backward draw（1回）
x_draw_1 <- ck_backward_sample_states_scalar(fwd)

data.frame(
  t = 1:n,
  x_true = x_true,
  x_draw = x_draw_1,
  y = y,
  K_true = K_true
)
```

---

## 6.3 複数ドローして（条件付き）事後平均の雰囲気を見る

```{r, eval = FALSE}
M <- 5000
Xdraw <- matrix(NA_real_, nrow = M, ncol = n)

for (j in 1:M) {
  Xdraw[j, ] <- ck_backward_sample_states_scalar(fwd)
}

post_mean <- colMeans(Xdraw)
post_sd <- apply(Xdraw, 2, sd)

data.frame(
  t = 1:n,
  x_true = x_true,
  post_mean = post_mean,
  post_sd = post_sd,
  y = y,
  K_true = K_true
)
```

---

# 7. 注意（Gibbs 全体にするには）

このRmdは「$K$ を与えた条件付きで $p(X \mid Y^n,K, \theta)$ から状態列を引く」部分だけを実装しています。  
Carter & Kohn の Gibbs 全体や Kim et al. のSV推計では、別途

- $p(K \mid Y^n,X, \theta)$ から $K$ を更新するステップ（離散）
- さらにパラメータ $\theta$ を更新するステップ

が加わります。
