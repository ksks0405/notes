---
title: "Carter & Kohn (1994)"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: show
---


# Settings

The chapter contains some chunks that conduct document settings.

-   This chunk **removes all the variables** in the environment.

```{r SET housekeeping, class.source='fold-hide'}
rm(list = ls(all = TRUE))
```

-   This chunk **sets** `knitr` **options**.

```{r SET knit options, class.source='fold-hide'}
knitr::opts_chunk$set(
  block.title = TRUE,
  fig.align = "center",
  results = "hold",
  fig.show = "hold",
  message = FALSE,
  warning = FALSE,
  class.source = 'fold-hide'
)
knitr::opts_hooks$set(label = function(options) {
  options$before = paste0('<div>Chunk: ', options$label, '</div>')
  return(options)
})
```

-   This chunk **loads the required packages** and **my functions**.

```{r SET packages, class.source='fold-hide'}
pkgList <- c(
  "tidyverse" #must-have
)
easypackages::libraries(pkgList)
rm(pkgList)
```

-   This chunk sets **the seed**.

```{r SET seeds, class.source='fold-hide'}
set.seed(1111)
```

# セットアップ（Carter & Kohn の記法）

Carter & Kohn (1994) の基本モデルは

$$
\begin{alignedat}{2}
y(t) &= h(t)^\prime x(t) + e(t), &\quad \text{(1-1)}\\
x(t+1) &= F(t+1)x(t) + u(t+1). &\quad\text{(1-2)}
\end{alignedat}
$$

- 観測：スカラー $y(t)$  

- 状態：$m \times 1$ の $x(t)$  

- indicator：$K(t)$（誤差の mixture 成分など）  

- $K = \{K(1), \ldots,K(n)\}$

を与えると、誤差が条件付きガウスになり、(1-1)(1-2) はガウス状態空間

# §2-2 の要点：後ろ向き分解（Lemma 2-1）

Lemma 2-1 は（$K,\theta$ を固定した条件付きの下で）

$$
p(X \mid Y^n)=p(x(n) \mid Y^n)\\  \prod_{t=1}^{n-1} p(x(t) \mid Y^t, x(t+1)),
$$

という分解を与えます。

実装は

1. **前向き**：Kalman filter で $x(t \mid t)$ と $S(t \mid t)$ を全時点保存

2. **後ろ向き**：

   - $x(n) \sim p(x(n) \mid Y^n)=N(x(n \mid n),S(n \mid n))$

   - $t=n-1, \ldots,1$ で $x(t) \sim p(x(t) \mid Y^t,x(t+1))$

です。

# FFBS数値例

## モデル

Carter & Kohn (1994) Chap.2をベースに、観測と状態がともにスカラーで、かつ観測係数と状態係数がともに定数（i.e. $F_t = F, \, h_t = h$）のモデルを考える：

$$
\begin{alignedat}{2}
y_t &= h\,x_t + e_t, && \qquad t=1,\ldots,n,\\
x_{t+1} &= F\,x_t + u_t,&& \qquad t=1,\ldots,n-1,
\end{alignedat}
$$

- $\{y_t\}_{t=1}^n$：スカラー観測、

- $\{x_t\}_{t=1}^n$：スカラー状態、

- $h$：観測方程式の係数、$e_t$：観測誤差、

- $F$：状態方程式の係数、$u_t$：状態イノベーション

- **状態イノベーション**は同分散な分散$\tau>0$を仮定：
  $$
  u_t \sim N(0,\tau),\qquad t=1,\ldots,n-1,
  $$

- **観測誤差**は**二成分の混合正規（mixture normal）**とし、基準分散 $R>0$ 、 スケール $C>1$、潜在indicator$K_t\in\{0,1\}$を使って以下のように表す：
  $$
  e_t \mid K_t=0 \sim N(0,R),\quad
  e_t \mid K_t=1 \sim N(0,CR), \quad \Pr(K_t=0)=p,\quad \Pr(K_t=1)=1-p.
  $$

また、初期状態（つまり$t = 1$の予測分布）は、$a_1$（初期平均）、$s_1$ （初期分散）を用いて以下のように仮定する。

$$
x_1 \sim N(a_1, s_1)
$$

さらに、条件付きでガウス状態空間になるように、$K=(K_1,\ldots,K_n)$ を与えたもとで
$\{e_t\}$ と $\{u_t\}$ は互いに独立、かつそれぞれ時点間独立であると仮定する。

$K_t$ を与えると観測誤差分散は時点ごとに
$$
R_t := \mathrm{Var}(e_t\mid K_t)=
\begin{cases}
R & (K_t=0),\\
CR & (K_t=1),
\end{cases}
$$
と確定し、モデルは $(h,F,R_t,\tau)$ を持つ線形ガウス状態空間モデルとして扱える。

## データ生成（fictitious data）

以下では、真のモデルを$h=1$、$F=0.7$、$R = 1$、$C = 4$としてデータを生成する。ただし、$n=4$のサンプルでは、数値例を簡潔にするため状態・観測ノイズを手動で設定する（真の$K$は$\{K_t\}_{t=1}^4 = \{0, 1, 0, 1\}$を想定）。

```{r IMP parameters}
# Model: x_{t+1} = 0.7 x_t + u_t,  y_t = x_t + e_t
# Var settings (model): Var(e_t)=1 or 4 depending on K, Var(u_t) = 1
n <- 4
h <- 1
FF <- 0.7
R <- 1
C <- 4
K <- c(0, 1, 0, 1)
R_t <- ifelse(K == 0, R, C * R)
tau <- 1
```

```{r IMP dataset}
# Fixed shocks
x1 <- 0
u  <- c(1, 2, -1)         # u1,u2,u3 (fixed)
e  <- c(0.5, -0.5, 1, -1) # e1..e4 (fixed)

# Build state and observation
x <- numeric(n)
x[1] <- x1
for(t in 1:(n-1)){
  x[t+1] <- FF * x[t] + u[t]
}
y <- h * x + e

truth <- data.frame(t = 1:n, K = K, x = x, y = y, e = e, 
                    u = c(u, NA), Var_e = R_t, Var_u = rep(tau, n))
```

```{r EXP dataset}
truth
```

### Forward filtering

forward filtering では、$x_t$**の1期前までの観測に基づく状態の予測分布**

$$ 
\begin{aligned}
x_{t}|Y^{t-1} &\sim N(a_{t}, s_{t}), \,\text{where}\\
a_{t} &= E[x_{t}|Y^{t-1}]\\
s_{t} &= E[(x_{t}-a_{t})^2|Y^{t-1}]
\end{aligned}
$$

を定義する。ここで $Y^{t-1}=(y_1,\ldots,y_{t-1})$ である。このとき、$y_1$**の予測分布**は、$y_t = hx_t + e_t,\ (e_t \sim \text{mixture})$より、

$$
y_t \mid Y^{t-1} \sim N(b_t, v_t),
$$

$$
b_t := E(y_t\mid Y^{t-1})=h\,a_t,\quad
v_t := \mathrm{Var}(y_t\mid Y^{t-1})=h^2 s_t + R_t
$$
である。

実現値との回帰

$$
\boxed{\underbrace{ x_{t}-E[x_{t}|Y^{t-1}] }_{x_{t} \text{の予測分布からの乖離をアップデート}}=L_{t}\underbrace{ (y_{t}-E[y_{t}|Y^{t-1}]) }_{y_{t} \text{の実現値からの乖離を基に}} + \eta_{t} }
$$
の両辺に$(y_{t}-b_{t})$をかけ期待値を取ると、

$$ 
\begin{alignedat}{2}
&E\Big[\underbrace{ (x_{t}-\underbrace{ E[x_{t}|Y^{t-1}] }_{=a_{t}})(y_{t}-\underbrace{ E[y_{t}|Y^{t-1}] }_{=b_{t} = h a_{t}}) }_{h(x_{t}-a_{t})(x_{t}-a_{t}+e_{t})}\Big] &&= L_{t}E\Big[(y_{t}-E[y_{t}|Y^{t-1}])^2\Big]\\
\Rightarrow & \qquad \qquad h \,\text{var}[x_{t}|Y^{t-1}] &&= L_{t}  \, \text{var}[y_{t}|Y^{t-1}]\\
\Rightarrow & \qquad \qquad hs_{t} &&= L_{t} (h^2s_{t}+R_{t})
\end{alignedat}
$$
$$
\therefore L_t =  \frac{hs_t}{h^2s_t+R_t}
$$

#### Forward sampling 実装 

以上のステップは、以下のforward filtering関数`ck_forward_filter_scalar_mixture`により実行可能。

```{r FUN ff}
ck_forward_filter_scalar_mixture <- function(y, h, F, R, C, K, tau, a1, s1) {
  # checks
 y <- as.numeric(y)
  n <- length(y)
  if (length(K) != n) stop("K must have length n.")
  if (any(!K %in% c(0,1))) stop("K must be 0/1.")
  if (R <= 0 || C <= 0 || tau <= 0 || s1 <= 0) stop("variances/scales must be > 0.")

  # results variable
  a <- numeric(n)  # a_t = E[x_t | y^{t-1}]
  s <- numeric(n)  # s_t = Var[x_t | y^{t-1}]
  b <- numeric(n)  # b_t = E[y_t | y^{t-1}]
  v <- numeric(n)  # v_t = Var[y_t | y^{t-1}]
  a_upd <- numeric(n)  # a_t^{+} = E[x_t | y^{t}]
  s_upd <- numeric(n)  # s_t^{+} = Var[x_t | y^{t}]
  gain <- numeric(n)　# Kalman gain
  
  # conditional obs variances R_t = Var(e_t | K_t)
  R_t <- ifelse(K == 0, R, C * R)

  ### MAIN PART ###
  for (t in 1:n) {
    ### prior predict of state x
    if(t == 1){
      # initial state for t=1
      a[t] <- a1
      s[t] <- s1
    }else{
      # update from t-1 filter distribution
      a[t] <- F * a_upd[t-1]
      s[t] <- (F^2) * s_upd[t-1] + tau
    }
    ### prior predict of observation y
    b[t] <- h * a[t]
    v[t] <- (h^2) * s[t] + R_t[t]

    ### calculate Kalman gain
    gain[t] <- (s[t] * h) / v[t]

    # update with y_t
    a_upd[t] <- a[t] + gain[t] * (y[t] - b[t])
    s_upd[t] <- s[t] - (gain[t]^2) * v[t]   # equivalent to (1 - gain[t]*h)*s[t]
  }

  list(
    n = n, y = y, h = h, F = F, R = R, C = C, K = K, tau = tau, 
    a1 = a1, s1 = s1, R_t = R_t, a = a, s = s, b = b, v = v, 
    a_upd = a_upd, s_upd = s_upd, gain = gain
  )
}
```

```{r REG ff using function, class.source='fold-show'}
fwd <- ck_forward_filter_scalar_mixture(
  y = y, h = h, F = FF, R = R, C = C, K = K, a1 = 0, s1 = 1, tau = tau)
fwd1 <- ck_forward_filter_scalar_mixture(
  y = y, h = h, F = FF, R = R, C = C, K = c(0, 0, 0, 0), a1 = 0, s1 = 1, tau = tau)
fwd2 <- ck_forward_filter_scalar_mixture(
  y = y, h = h, F = FF, R = R, C = C, K = c(1, 1, 1, 1), a1 = 0, s1 = 1, tau = tau)
```

$t = 1$ の場合について数値例を基に手計算すると、

$$
L_1 = \frac{hs_1}{h^2s_1+R_1} = \begin{cases}
\dfrac{1 \times 1}{1^2\times 1+1} = 0.5 \quad (\text{low volatility})\\[1em]
\dfrac{1 \times 1}{1^2\times 1+4} = 0.2 \quad (\text{high volatility})
\end{cases}
$$

$$
a_1^+ := \underbrace{ E[{x}_1|Y^0] }_{a_1}+L_1\underbrace{(y_1-E[y_1|Y^0])}_{y_1-ha_1} = \begin{cases}
0 + 0.5 \times (0.5-0) = 0.25 \quad (\text{low volatility})\\[1em]
0 + 0.2 \times (0.5-0) = 0.10 \quad (\text{high volatility})
\end{cases}
$$

となり、手計算と関数が一致することが確認できる。



```{r REG ff function vs handroll, class.source='fold-show'}
print("手計算 (high; low)")
0+(h)/((h)^2+R)*(y[1]-0)
0+(h)/((h)^2+R*C)*(y[1]-0)
print("関数 (high; low)")
fwd1$a_upd[[1]]
fwd2$a_upd[[1]]
```

以下は、$h, F, C, R$の値を真の値で既知とし、2つの異なる$\{K\}$を仮定してフィルタリング分布の平均値を示している。$t = 1, 2, 3$を見ると、priorより大きい観測値を受けて、観測誤差のノイズを小さく見積もっている$\{K\}=\{0,0,0,0\}$ケースでより大きくposteriorを修正している。

```{r EXP ff plot}
# --- 1) プロット用データ ---
mk_df <- function(fwd, case_label) {
  tibble(
    t = 1:fwd$n,
    case = case_label,
    prior = fwd$a,          # a_t = E[x_t | y^{t-1}]
    posterior = fwd$a_upd   # a_t^{+} = E[x_t | y^{t}]
  )
}

df12 <- bind_rows(mk_df(fwd1, "K=(0,0,0,0)"),  mk_df(fwd2, "K=(1,1,1,1)")) %>%
  pivot_longer(cols = c(prior, posterior), names_to = "series", values_to = "value") %>%
  mutate(
    series = factor(series, levels = c("prior", "posterior"))
  )

# --- 2) 観測値 ---
df_obs <- tibble(t = 1:length(y), y = as.numeric(y))

# --- 3) ggplot ---
ggplot() +
  # 観測値：×（線なし）
  geom_point(
    data = df_obs,
    aes(x = t, y = y),
    shape = 4, size = 3, stroke = 1
  ) +
  # prior / posterior：ケースで色分け、priorは点線、posteriorは実線
  geom_line(
    data = df12,
    aes(x = t, y = value, color = case, linetype = series)
  ) +
  scale_linetype_manual(values = c(prior = "dashed", posterior = "solid")) +
  labs(
    x = "t",
    y = "value",
    color = "Case (K)",
    linetype = "",
    title = "Prior (a_t) and Posterior (a_t^{+}) vs Observations"
  ) +
  theme_minimal()

```

### Backward smoothing / sampling

$x_T$の事後予測（$x_T|y^T \sim N(a_T^{+}, s_T^{+}) $）は、**forward filteringの最終ステップ**から直ちに得られる。

**Backward smoothing**では、$x_{T-1}$のフィルタリング分布（$x_{T-1}|y_{T-1}$）を、$x_T|y_{T-1}$と$x_T$の差（すなわち$T-1$期時点の予測分布からの乖離＝$T$時点の情報を反映）に回帰。これにより、**フィルタリング分布に逐次的に未来の情報を取り込んでいく**：

$$
\boxed{ \underbrace{x_{T-1}-E[x_{T-1}|y^{T-1}] }_{x_{T-1}\text{のフィルタ分布からの乖離をアップデート}} =  J_{t-1}\underbrace{(x_T- E[x_T|y^{T-1}]) }_{x_T\text{の予測分布からの乖離を基に}}+\xi_t}
$$

両辺に$(x_T-E[x_T|y^{T-1}])$をかけ期待値を取ると、
$$
\begin{aligned}
E\Big[\underbrace{ (x_{T-1}-\underbrace{ E[x_{T-1}|y^{T-1} }_{=a_{T-1}^+}])(x_T-\underbrace{ E[x_T|y^{T-1}] }_{Fa_{T-1}^+}) }_{F(x_{T-1}-a_{T-1}^+)(x_{T-1}-a_{T-1}^+ + u_{T-1})}\Big] &=  J_{t-1}E\Big[(x_T-E[x_T|y^{T-1}])^2\Big]\\
\Rightarrow F \,\text{var}[x_{T-1}|y^{T-1}] &=J_{t-1} \text{var}[x_T|y^{{T-1}}]\\
\Rightarrow J_{t-1}  &= \frac{F s_{T-1}^+}{F^2s_{T-1}^+ + \tau}\\
\end{aligned}
$$

$$
\therefore J_{t-1}  = \frac{F s_{T-1}^+}{F^2s_{T-1}^+ + \tau} 
$$

#### Backward smoothing 実装

Backward smoothingを実装する関数は以下の通り。

```{r FUN backward smoothing}
ck_backward_smoother_scalar <- function(fwd) {
  # fwd: output of ck_forward_filter_scalar_mixture()
  n   <- fwd$n
  F   <- fwd$F
  tau <- fwd$tau

  a_filt <- fwd$a_upd   # a_t^+ = E[x_t | y^t]
  s_filt <- fwd$s_upd   # s_t^+ = Var[x_t | y^t]

  # One-step-ahead predictions from filtered moments:
  # a_{t+1|t} = F a_t^+
  # s_{t+1|t} = F^2 s_t^+ + tau
  a_pred_next <- rep(NA_real_, n)
  s_pred_next <- rep(NA_real_, n)
  for (t in 1:(n - 1)) {
    a_pred_next[t] <- F * a_filt[t]
    s_pred_next[t] <- (F^2) * s_filt[t] + tau
  }

  # RTS smoothing gain J_t
  J <- rep(NA_real_, n)
  for (t in 1:(n - 1)) {
    J[t] <- (s_filt[t] * F) / s_pred_next[t]
  }

  # Smoothed moments
  a_smooth <- rep(NA_real_, n)  # E[x_t | y^n]
  s_smooth <- rep(NA_real_, n)  # Var[x_t | y^n]

  # Initialize at terminal time: smoothing = filtering
  a_smooth[n] <- a_filt[n]
  s_smooth[n] <- s_filt[n]

  # Backward recursion
  if (n >= 2) {
    for (t in (n - 1):1) {
      a_smooth[t] <- a_filt[t] + J[t] * (a_smooth[t + 1] - a_pred_next[t])
      s_smooth[t] <- s_filt[t] + (J[t]^2) * (s_smooth[t + 1] - s_pred_next[t])
      s_smooth[t] <- max(s_smooth[t], 0)  # numerical safety
    }
  }

  list(
    n = n,
    a_smooth = a_smooth,
    s_smooth = s_smooth,
    J = J,
    a_filt = a_filt,
    s_filt = s_filt,
    a_pred_next = a_pred_next,
    s_pred_next = s_pred_next
  )
}

```

```{r REG backward smoothing}
sm1 <- ck_backward_smoother_scalar(fwd1)
sm2 <- ck_backward_smoother_scalar(fwd2)
```

```{r}
# --- 4) backward smoother の点予測（E[x_t|y^T]）を df に ---
mk_sm_df <- function(sm, case_label) {
  tibble(
    t = 1:sm$n,
    case = case_label,
    smooth = sm$a_smooth   # E[x_t | y^T]
  )
}

df_sm <- bind_rows(
  mk_sm_df(sm1, "K=(0,0,0,0)"),
  mk_sm_df(sm2, "K=(1,1,1,1)")
)

# --- 5) 既存プロットに追加（caseで色は合わせ、線種だけ別にする） ---
ggplot() +
  # 観測値：×（線なし）
  geom_point(
    data = df_obs,
    aes(x = t, y = y),
    shape = 4, size = 3, stroke = 1
  ) +
  # prior / posterior：ケースで色分け、priorは点線、posteriorは実線
  geom_line(
    data = df12,
    aes(x = t, y = value, color = case, linetype = series),
    linewidth = 0.9
  ) +
  scale_linetype_manual(
    values = c(prior = "dotted", posterior = "dashed", smooth = "solid")
  ) +
  geom_line(
    data = df_sm,
    aes(x = t, y = smooth, color = case, linetype = "smooth"),
    linewidth = 1.1
  ) +
  labs(
    x = "t",
    y = "value",
    color = "Case (K)",
    linetype = "",
    title = "Prior (a_t), Posterior (a_t^{+}), and Backward Smoother E[x_t|y^T] vs Observations"
  ) +
  theme_minimal()

```

Smootherは、posteriorよりスムーズ。

#### Backward sampling 実装 

```{r FUN backward sampling}
ck_backward_sampler_scalar <- function(fwd) {
  # Draw x_{1:n} ~ p(x_{1:n} | y^n, K, theta) under linear-Gaussian (K fixed)
  n   <- fwd$n
  F   <- fwd$F
  tau <- fwd$tau

  a_filt <- fwd$a_upd
  s_filt <- fwd$s_upd

  # One-step-ahead prediction moments from filtered moments:
  a_pred_next <- rep(NA_real_, n)
  s_pred_next <- rep(NA_real_, n)
  for (t in 1:(n - 1)) {
    a_pred_next[t] <- F * a_filt[t]
    s_pred_next[t] <- (F^2) * s_filt[t] + tau
  }

  # Smoothing gain J_t = Cov(x_t,x_{t+1}|y^t) / Var(x_{t+1}|y^t)
  J <- rep(NA_real_, n)
  for (t in 1:(n - 1)) {
    J[t] <- (s_filt[t] * F) / s_pred_next[t]
  }

  x <- rep(NA_real_, n)

  # 1) Draw terminal state from filtering distribution
  x[n] <- rnorm(1, mean = a_filt[n], sd = sqrt(s_filt[n]))

  # 2) Draw backward: x_t | (y^t, x_{t+1})
  if (n >= 2) {
    for (t in (n - 1):1) {
      # Conditional mean: a_t^+ + J_t (x_{t+1} - a_{t+1|t})
      mean_t <- a_filt[t] + J[t] * (x[t + 1] - a_pred_next[t])

      # Conditional variance: s_t^+ - J_t^2 * Var(x_{t+1}|y^t)
      var_t  <- s_filt[t] - (J[t]^2) * s_pred_next[t]
      var_t  <- max(var_t, 0)  # numerical safety

      x[t] <- rnorm(1, mean = mean_t, sd = sqrt(var_t))
    }
  }

  list(
    x_draw = x,
    J = J,
    a_filt = a_filt,
    s_filt = s_filt,
    a_pred_next = a_pred_next,
    s_pred_next = s_pred_next
  )
}
```

```{r}
# 1) Backward smoothing (RTS): E[x_t | y^T] and +/- 1 sd band
sm <- ck_backward_smoother_scalar(fwd)

df_band <- tibble(
  t = 1:sm$n,
  mean = sm$a_smooth,
  sd   = sqrt(sm$s_smooth),
  lo   = mean - sd,
  hi   = mean + sd
)

# 2) Backward sampling: 10000 sample paths
M <- 10000
Xdraw <- replicate(M, ck_backward_sampler_scalar(fwd1)$x_draw)  # n x M

df_draws <- as_tibble(Xdraw[1:n, 1:50]) %>%
  mutate(t = 1:sm$n) %>%
  pivot_longer(
    cols = -t,
    names_to = "draw",
    values_to = "x"
  ) %>%
  mutate(draw = factor(draw))

# 3) Plot: band + smoothed mean + 50 sampled trajectories
ggplot() +
  geom_ribbon(data = df_band, aes(x = t, ymin = lo, ymax = hi), alpha = 0.2) +
  geom_line(data = df_band, aes(x = t, y = mean), linewidth = 1.1) +
  geom_line(data = df_draws, aes(x = t, y = x, group = draw), linewidth = 0.5, alpha = 0.5) +
  labs(
    x = "t", y = "state", 
    title = "Backward smoothing: E[x_t | y^T] ± 1 SD, and 50 backward-sampling draws"
  )
```

**注意点：SamplingとSmoothingの違い**

- Samplingは、$x_T$（サンプル値）を使う

- Smoothingは、$\tilde{a}_T = E[x_T|y^T]$を使う

以下のグラフからわかる通り、ここの（marginal）smoothing分布は、$a^{++}_t=E[x_t|Y^T]$と$s^{++}_t=\text{var}[x_t|Y^T]$の正規分布として与えられるが、同時分布は（一般に）相関することがある。

```{r EXP marginal vs joint distribution}
# --- 1) Build theoretical (smoothed) marginal densities for x1, x2 ---
mu1 <- sm1$a_smooth[1]
sd1 <- sqrt(sm1$s_smooth[1])

mu2 <- sm1$a_smooth[2]
sd2 <- sqrt(sm1$s_smooth[2])

x1_emp <- as.numeric(Xdraw[1, ])
x2_emp <- as.numeric(Xdraw[2, ])

make_theory_df <- function(mu, sd, emp_vec, varname) {
  # grid wide enough to show both theory and empirical
  lo <- min(quantile(emp_vec, 0.005), mu - 4 * sd)
  hi <- max(quantile(emp_vec, 0.995), mu + 4 * sd)
  grid <- seq(lo, hi, length.out = 400)

  tibble(
    var = varname,
    x = grid,
    density = dnorm(grid, mean = mu, sd = sd),
    source = "Smoothed N(a_smooth, s_smooth)"
  )
}

df_theory <- bind_rows(
  make_theory_df(mu1, sd1, x1_emp, "x1"),
  make_theory_df(mu2, sd2, x2_emp, "x2")
)

# empirical draws in long format
df_emp <- tibble(
  x1 = as.numeric(x1_emp),
  x2 = as.numeric(x2_emp)
) |>
  pivot_longer(cols = everything(), names_to = "var", values_to = "x") |>
  mutate(source = "FFBS empirical")

# --- 2) Plot: marginal comparison (x1 and x2), same graph per variable ---
p_marginals <- ggplot() +
  geom_density(
    data = df_emp,
    aes(x = x, linetype = source),
    linewidth = 0.9
  ) +
  geom_line(
    data = df_theory,
    aes(x = x, y = density, linetype = source),
    linewidth = 1.0
  ) +
  facet_wrap(~ var, scales = "free") +
  labs(
    x = "value",
    y = "density",
    linetype = "",
    title = "Marginals: Smoothed N(a_smooth, s_smooth) vs FFBS empirical"
  )

print(p_marginals)

# --- 3) Plot: empirical joint distribution of (x1, x2) ---
df_joint <- tibble(x1 = x1_emp, x2 = x2_emp)

corr12 <- cor(df_joint$x1, df_joint$x2)

p_joint <- ggplot(df_joint, aes(x = x1, y = x2)) +
  geom_point(alpha = 0.08, size = 0.6) +
  stat_density_2d(aes(fill = after_stat(level)), geom = "polygon", alpha = 0.25, color = NA) +
  labs(
    x = expression(x[1]),
    y = expression(x[2]),
    title = "Empirical joint distribution from FFBS draws (x1 vs x2)",
    subtitle = paste0("Empirical corr(x1, x2) = ", sprintf("%.4f", corr12))
  ) +
  guides(fill = "none")

print(p_joint)

```

# Gibbs Samling

## 長いデータ

```{r IMP long data}
pK1 <- 0.5

n_long <- 150

K_long <- rbinom(n_long, size = 1, prob = pK1)
Var_e_long <- ifelse(K_long == 0, R, C * R)
e_long <- rnorm(n_long, mean = 0, sd = sqrt(Var_e_long))
u_trans <- rnorm(n_long - 1, mean = 0, sd = sqrt(tau))

# build state x_t
x_long <- numeric(n_long)
x_long[1] <- 0  # 必要なら rnorm(1,0,sqrt(s1)) などに変更。

for (t in 1:(n_long - 1)) {
  x_long[t + 1] <- FF * x_long[t] + u_trans[t]
}

# observation y_t
y_long <- h * x_long + e_long

# pack as data.frame (matching your short truth format)
truth_long <- data.frame(
  t = 1:n_long,
  K = K_long,
  x = x_long,
  y = y_long,
  e = e_long,
  u = c(u_trans, NA),        # u_t is used for x_{t+1}; last is NA
  Var_e = Var_e_long,
  Var_u = rep(tau, n_long)
)
```

```{r EXP long data}
ggplot(data = truth_long, mapping = aes(x = t))+
  geom_line(mapping = aes(y = y), color = "red") +
  geom_line(mapping = aes(y = x), color = "blue", linetype = "dashed")
```

```{r}
sample_K_given_x <- function(y, x, h, R, C, p) {
  # returns K in {0,1}^n
  n <- length(y)
  e <- y - h * x

  # log weights
  logw0 <- log(p)     + dnorm(e, mean=0, sd=sqrt(R),   log=TRUE)
  logw1 <- log(1 - p) + dnorm(e, mean=0, sd=sqrt(C*R), log=TRUE)

  # P(K=1 | ...) = w1 / (w0+w1) in log-scale
  m <- pmax(logw0, logw1)
  w0 <- exp(logw0 - m)
  w1 <- exp(logw1 - m)
  prob1 <- w1 / (w0 + w1)

  rbinom(n, size=1, prob=prob1)
}
```

```{r}
sample_tau_given_x <- function(x, F, alpha0, beta0) {
  u <- x[2:length(x)] - F * x[1:(length(x)-1)]
  alpha_post <- alpha0 + (length(u))/2
  beta_post  <- beta0  + 0.5 * sum(u^2)

  # if 1/tau ~ Gamma(alpha_post, rate=beta_post)
  1 / rgamma(1, shape=alpha_post, rate=beta_post)
}
```

```{r}
gibbs_ssm_mixture_tau_xK <- function(y, h, F, R, C, p, a1, s1,
                                    alpha0=2, beta0=1,
                                    n_iter=5000, burn=1000,
                                    init_tau=1, init_K=NULL, seed=123) {
  set.seed(seed)
  y <- as.numeric(y)
  n <- length(y)

  if (is.null(init_K)) init_K <- rbinom(n, 1, 1-p)
  if (length(init_K) != n) stop("init_K must have length n.")
  K <- init_K
  tau <- init_tau

  # initial x draw
  fwd <- ck_forward_filter_scalar_mixture(y, h, F, R, C, K, tau, a1, s1)
  x <- ck_backward_sampler_scalar(fwd)$x_draw

  # storage
  keep <- (burn+1):n_iter
  n_keep <- length(keep)
  tau_draw <- numeric(n_keep)
  x_draw   <- matrix(NA_real_, nrow=n_keep, ncol=n)
  K_draw   <- matrix(NA_integer_, nrow=n_keep, ncol=n)

  idx <- 0
  for (iter in 1:n_iter) {

    # 1) x | K, tau, y  (FFBS)
    fwd <- ck_forward_filter_scalar_mixture(y, h, F, R, C, K, tau, a1, s1)
    x <- ck_backward_sampler_scalar(fwd)

    # 2) K | x, y
    K <- sample_K_given_x(y, x, h, R, C, p)

    # 3) tau | x
    tau <- sample_tau_given_x(x, F, alpha0, beta0)

    # store
    if (iter %in% keep) {
      idx <- idx + 1
      tau_draw[idx] <- tau
      x_draw[idx, ] <- x
      K_draw[idx, ] <- K
    }
  }

  list(
    y=y, h=h, F=F, R=R, C=C, p=p, a1=a1, s1=s1,
    alpha0=alpha0, beta0=beta0,
    tau_draw=tau_draw,
    x_draw=x_draw,
    K_draw=K_draw
  )
}
```

```{r}
a1 = 0
s1 = 1
alpha0 = 2
beta0  = 1
n_iter = 5000
burn   = 1000

y <- as.numeric(truth_long$y)
n <- length(y)

init_K <- rbinom(n, 1, 1-pK1)
init_tau <- 1

K <- init_K
tau <- init_tau

# initial x draw
fwd <- ck_forward_filter_scalar_mixture(y, h, FF, R, C, K, tau, a1, s1)
x <- ck_backward_sampler_scalar(fwd)

# storage
keep <- (burn+1):n_iter
n_keep <- length(keep)
tau_draw <- numeric(n_keep)
x_draw   <- matrix(NA_real_, nrow=n_keep, ncol=n)
K_draw   <- matrix(NA_integer_, nrow=n_keep, ncol=n)

idx <- 0
for (iter in 1:n_iter) {
  
  # 1) x | K, tau, y  (FFBS)
  fwd <- ck_forward_filter_scalar_mixture(y, h, FF, R, C, K, tau, a1, s1)
  x <- ck_backward_sampler_scalar(fwd)$x_draw
  
  # 2) K | x, y
  K <- sample_K_given_x(y, x, h, R, C, pK1)
  
  # 3) tau | x
  tau <- sample_tau_given_x(x, FF, alpha0, beta0)
  
  # store
  if (iter %in% keep) {
    idx <- idx + 1
    tau_draw[idx] <- tau
    x_draw[idx, ] <- x
    K_draw[idx, ] <- K
  }
}

hist(tau_draw)
```


```{r}
ggplot(data = truth_long, mapping = aes(x = t))+
  geom_line(mapping = aes(y = y), color = "red") +
  geom_line(mapping = aes(y = colMeans(x_draw)), color = "green") +
  geom_line(mapping = aes(y = x), color = "blue", linetype = "dashed")
```


```{r, class.source='fold-show'}
getwd()
```

```{r}

df_band_x <- tibble(
  t = 1:ncol(x_draw),
  q10 = apply(x_draw, 2, quantile, probs = 0.10, na.rm = TRUE),
  q50 = apply(x_draw, 2, quantile, probs = 0.50, na.rm = TRUE),
  q90 = apply(x_draw, 2, quantile, probs = 0.90, na.rm = TRUE),
  x_true = truth_long$x
)

ggplot(df_band_x, aes(x = t)) +
  geom_ribbon(aes(ymin = q10, ymax = q90), alpha = 0.25) +
  # geom_line(aes(y = q50), linewidth = 0.9) +
  geom_line(aes(y = x_true), linewidth = 0.8, linetype = "dashed") +
  labs(
    x = "t",
    y = "state x",
    title = "State: posterior median & 10–90% band (from x_draw) vs true state",
    subtitle = "Solid = posterior median, ribbon = 10–90%, dashed = true state"
  )
```
