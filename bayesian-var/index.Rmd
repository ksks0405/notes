---
title: "Bayesian VARs"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: show
editor_options: 
  chunk_output_type: console
---

<!-- The chunks in this code are classified as follows: -->

<!-- -   SET: Load package, set default options, define functions and variables, etc. -->

<!-- -   IMP: Import data. -->

<!-- -   PRE: Process data, including creating and editing variables, -->

<!-- -   FLT: Process data that involve FILTERING. -->

<!-- -   EDA: Exploratory Data Analysis -->

<!-- -   REG: Regressions -->

<!-- -   EXP: Export results -->

# Settings

The chapter contains some chunks that conduct document settings.

-   This chunk **removes all the variables** in the environment.

```{r SET housekeeping, class.source='fold-hide'}
rm(list = ls(all = TRUE))
```

-   This chunk **sets** `knitr` **options**.

```{r SET knit options, class.source='fold-hide'}
knitr::opts_chunk$set(
  block.title=T, # default: F / normally F
  fig.align = "center",
  results = "hold",
  fig.show = "hold",
  message = F, # when hiding messages
  warning = F  # when hiding errors
)
knitr::opts_hooks$set(label = function(options) {
  options$before = paste0('<div>Chunk: ', options$label, '</div>')
  return(options)
})
```

-   This chunk **loads the required packages** and **my functions**.

```{r SET libraries, class.source='fold-hide'}
pkgList <- c(
  "tidyverse", #must-have
  "ggplots", # read xlsx
  "plm",
  "modelsummary",
  "gridExtra",
  "plm",
  "pgmm"
)

easypackages::libraries(pkgList)

rm(pkgList)

# pkgList <- c(
#   "tidyverse", #must-have
#   "vars", # for VAR
#   "zoo", # table for VAR
#   "modelsummary", # summary functions and reg. tables
#   "quantmod" # FRED
# )
# easypackages::libraries(pkgList)
# rm(pkgList)
```

-   This chunk sets **the seed**.

```{r SET seeds, class.source='fold-hide'}
set.seed(1111)
```

# データ

## Import and Pre-process

FREDからDL

```{r IMP dataset}
getSymbols(c("GDPC1", "GDPDEF", "FEDFUNDS"), src = "FRED")
# GDP, def -> quarterly
rgdp_q <- zoo(as.numeric(GDPC1), as.yearqtr(index(GDPC1)))
def_q  <- zoo(as.numeric(GDPDEF), as.yearqtr(index(GDPDEF)))

# FF -> monthly avg
ff_m <- zoo(as.numeric(FEDFUNDS), as.yearmon(index(FEDFUNDS)))
ff_q <- aggregate(ff_m, as.yearqtr, mean)

# merge
z_q <- na.omit(merge(rgdp_q, def_q, ff_q))
colnames(z_q) <- c("RGDP", "DEF", "FF")

# tibble
df_var <- tibble(
  date  = as.Date(as.yearqtr(index(z_q)), frac = 1),  # quarter end date
  lRGDP = 4*log(as.numeric(z_q[, "RGDP"])),
  lDEF  = 4*log(as.numeric(z_q[, "DEF"])),
  FF    = as.numeric(z_q[, "FF"])
)
```


```{r FLT span}
# 推計期間
df_var <- filter(df_var, "1995-01-01" <= date, date <= "2019-12-31")
```

dfをtsに変換する関数

```{r FUN df to ts, class.source='fold-hide'}
to_ts_quarterly <- function(df, cols = c("lRGDP", "lDEF", "FF")) {
  stopifnot(all(c("date", cols) %in% names(df)))
  df <- df %>% arrange(date)

  q <- as.yearqtr(df$date)

  y0 <- as.integer(floor(as.numeric(q[1])))
  qtr0 <- as.integer(round((as.numeric(q[1]) - y0) * 4 + 1))

  ts_out <- ts(as.matrix(df[, cols]), start = c(y0, qtr0), frequency = 4)
  colnames(ts_out) <- cols
  ts_out
}
```

```{r EXP dataset}
df_var
```

# 推計

## OLS

ラグ・データセットの設定

```{r REG setting}
p <- 5
ts_var <- to_ts_quarterly(df_var)
```

`vars::VAR`を利用

```{r REG var}
var_est <- VAR(ts_var, p = p, type = "const")
```

Quick check of result;

```{r EXP ols result}
coef_order <- c(
  unlist(lapply(1:p, function(l) paste0("lRGDP.l", l))),
  unlist(lapply(1:p, function(l) paste0("lDEF.l",  l))),
  unlist(lapply(1:p, function(l) paste0("FF.l",    l))),
  "const"
)

msummary(var_est$varresult, stars = T, coef_map = coef_order, fmt = 3, statistic = "std.error",
         title = "Reduced-form VAR Estimation Results")

# stationary check
print("Roots")
roots(var_est)
```

## ベイズ推定

**スカラー**

$M$ 変数の VAR($p$) モデルは次のように書ける：

$$
y_t = a_0 + \sum_{j=1}^{p} A_j y_{t-j} + \varepsilon_t \qquad (2.1)
$$

ここで、

- $y_t$ は $M \times 1$ の内生変数ベクトル
- $a_0$ は $M \times 1$ の定数項
- $A_j$ は $M \times M$ の係数行列
- 誤差項は $\varepsilon_t \stackrel{i.i.d.}{\sim} N(0, \Sigma)$。

**行列形式**

各時点の回帰変数を $x_t = (1, y_{t-1}', \ldots, y_{t-p}')'$ と定義する。これを $T$ 期間分スタックして、

$$
X =\begin{bmatrix} x_1'\\ x_2'\\ \vdots\\ x_T' \end{bmatrix} \qquad (2.2)
$$
とする。また、以下を定義する：

- $K = 1 + Mp$（各方程式の説明変数の数）
- $A = (a_0, A_1, \ldots, A_p)$：$K \times M$ の係数行列
- $Y$ は $T \times M$ の被説明変数行列
- $E$ は対応する誤差行列

$T$ 期間のデータをまとめると、VAR は次のように書ける：

$$
\underbrace{Y}_{T \times M} = \underbrace{X}_{T \times K} \underbrace{A}_{K \times M} + E \qquad (2.3)
$$

**vec形式**

式 (2.3) をベクトル化すると、

$$
y = \underbrace{(I_M \otimes X)}_{\begin{equation}\quad \,\,\, TM \times KM \\ \begin{bmatrix}X & 0 & \dots & 0\\  0 & X &\dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & X \end{bmatrix} \end{equation}}\underbrace{\alpha}_{KM \times 1} + \varepsilon \qquad (2.4)
$$
と書ける。ここで、

- $y = \mathrm{vec}(Y)$
- $\alpha = \mathrm{vec}(A)$：$KM \times 1$ の係数ベクトル
- $\varepsilon = \mathrm{vec}(E)$
- 誤差項は $\varepsilon \sim N(0, \Sigma \otimes I_T)$。

以下では、行列形式・vec形式のデータを定義している。

- ラグ数（$p$）、内生変数の数（$M$）、説明変数の数（$K \equiv pM + 1$）、時系列観測数（$T$ or `TT`）

```{r SET parameter, class.source = 'fold-show'}
p <- p # num of lags
M <- 3 # num of vars
K <- 1+M*p # num of total endos
TT <- NROW(df_var)-p # num of time obs.
```

- $Y \,(T \times M)$、$X \,(T \times K)$、$y \, (TM \times 1)$、$I_M \otimes X \, (TM \times KM)$

`make_varmat_KK`関数の定義

```{r FUN make variant kk, class.source = 'fold-show'}
make_varmat_KK <- function(df, vars, p, add_const = TRUE) {
  stopifnot(p >= 1)
  stopifnot(all(vars %in% colnames(df)))

  T_df <- NROW(df)
  M <- length(vars)

  # Y: (T-p) x M  (drop first p observations)
  Y <- df[(p + 1):T_df, vars, drop = F] %>% as.matrix()
  # y = vec(Y) in column-major order
  y <- as.vector(Y)

  # X: (T-p) x K  (const + lagged regressors)
  X_list <- lapply(1:p, function(lag) {
    tmp <- df[(p + 1 - lag):(T_df - lag), vars, drop = F]
    colnames(tmp) <- paste0(vars, ".l", lag)
    tmp
  })
  X <- dplyr::bind_cols(X_list)
  if (add_const) X <- dplyr::bind_cols(const = 1, X)
  X <- as.matrix(X)
  # Xbig = I_M kron X
  I_M <- diag(M)
  Xbig <- kronecker(I_M, X)

  list(Y = Y, y = y, X = X, Xbig = Xbig)
}
```

実行・確認

```{r SET data matrix, class.source = 'fold-show'}
vm <- make_varmat_KK(df_var, vars = c("lRGDP", "lDEF", "FF"), p = p, add_const = TRUE)

Y <- vm$Y; X <- vm$X; y <- vm$y; Xbig <- vm$Xbig
```

```{r EXP dimension}
print("dim Y \n"); print(dim(Y)) 
print("dim X \n"); print(dim(X)) 
print("dim y \n"); print(dim(y))
print("dim Im Kron X \n"); print(dim(Xbig)) 
```

```{r REG ols result}
A_OLS <- solve(t(X) %*% X) %*% t(X) %*% Y
alpha_OLS <- solve(t(Xbig) %*% Xbig) %*% t(Xbig) %*% y
```

### ミネソタ事前分布

モデル再掲：
$$
y = (I_M \otimes X)\alpha + \varepsilon, \qquad \varepsilon \sim N(0, \Sigma \otimes I_T)
$$

Minnesota Prior では、係数ベクトル $\alpha \, (KM \times 1)$ に対して次の正規事前分布を仮定する：

$$
\alpha|\Sigma \sim N(\underline{\alpha}_{Mn}, \underline{V}_{Mn}), 
$$

- $\underline{\alpha}$ は事前平均ベクトル
- $\underline{V}$ は事前分散行列（通常は対角行列


#### Known variance

Minnesota Prior では、誤差項の共分散行列は既知とする。ここでは、OLS推定値を使用する。

$$
\Sigma = \hat{\Sigma}
$$

```{r SET minnesota sigma}
Omega_Mn <- summary(var_est)$covres
```

```{r EXP minnesota sigma}
print("Omega_Mn")
as_tibble(Omega_Mn, rownames = " ")
```

#### Prior on coefficients

係数の事前平均は、次のように設定される。

> Thus, $\underline{\alpha}_{Mn} = 0_{KM}$ except for the elements corresponding to the first own lag of the dependent variable in each equation. These elements are set to one. These are the traditional choices for $\underline{\alpha}_{Mn}$, but anything is possible.

すなわち、事前平均ベクトル $\underline{\alpha}_{Mn}$は基本的にゼロベクトルであるが、**各方程式における自分自身の第1ラグの係数のみ 1** に設定される。

係数 $\beta_{i,j,l}$（第 $i$ 方程式における、第 $j$ 変数の第 $l$ ラグ）について書けば、

$$
E(\beta_{i,j,1}) =
\begin{cases}
1 & (i=j) \\
0 & (i\neq j)
\end{cases},
\qquad
E(\beta_{i,j,l}) = 0 \quad (l \ge 2)
$$

となる。

#### 事前分散（prior variance）

Koop and Korobilis (2010) の式 (2.8)に従い、係数の事前分散は次のように設定される。

$$
\underline{V}_{Mn, i,jj} = \begin{cases}
\displaystyle \frac{a_1^2}{r^2}
& \text{for coefficiencts on own lag } r \text{ for } r = 1, \dots, p  \\[1em]
\displaystyle \frac{a_2^2}{r^2}
\frac{\sigma_i^2}{\sigma_j^2}
& \text{for coefficiencts on lag } r \text{ of variable } j \neq i \\
&\text{ for }r = 1, \dots, p\\[1em]
a_3^2 \sigma_i^2 
& \text{for coefficients on exogenous variables}
\end{cases}
\qquad (2.8)
$$

各ハイパーパラメータの意味は以下の通りである。

- $a_1$：**自己ラグ（own lag）** に対する縮小パラメータ  
- $a_2$：**他変数ラグ（cross lag）** に対する縮小パラメータ  
- $a_3$：**定数項**に対する縮小パラメータ
- $r$：ラグの次数  
- $\sigma_i^2$：方程式 $i$ の誤差分散

この分散構造により、高次ラグほど、また他変数のラグほど、係数はより強くゼロへと縮小される。


```{r REG minnesota prior, class.source = 'fold-show'}
### hyperparamter
a1 <- 1
a2 <- 0.5
a3 <- 0.1

### alpha
alpha_Mn <- matrix(0, M*K, 1)
alpha_Mn[c(0*K+1+1, 1*K+2+1, 2*K+3+1)] <- 1

### V
V_Mn_diag <- rep(NA, M*K)

# lag decay part
V_Mn_r <- c(1, rep((1:p)^(-2), each = M)) 
# a and sigma part
for (i in 1:M) {
  V_Mn_asigma <- rep(NA_real_, K)
  
  V_Mn_asigma[1] <- a3 * Omega_Mn[i,i] # constant

  for (j in 1:M) {
    idx_j <- 1 + j + (0:(p-1)) * M # positions of variable j
    if (i == j) {
      V_Mn_asigma[idx_j] <- a1
    } else {
      V_Mn_asigma[idx_j] <- a2 * Omega_Mn[i,i] / Omega_Mn[j,j]
    }
  }
  V_Mn_diag[(1 + (i-1)*K):(i*K)] <- V_Mn_r * V_Mn_asigma
  
}

V_Mn <- diag(V_Mn_diag)
```

#### 事後分布

誤差分散共分散行列 $\Sigma$ が既知のため、Minnesota prior と正規尤度の共役性により、事後分布は正規分布となる。

$$
\alpha \mid y, \Sigma \sim N\!\left( \bar{\alpha}_{Mn}, \bar{V}_{Mn} \right)
$$

ここで、事後分散行列は

$$
\bar{V}_{Mn} = \left( \underline{V}_{Mn}^{-1} + \Sigma^{-1} \otimes X'X \right)^{-1}
$$

```{r REG minnesota v, class.source = 'fold-show'}
V_Mn_post <- solve(solve(V_Mn)+kronecker(solve(Omega_Mn), t(X) %*% X))
```

事後平均は

$$
\bar{\alpha}_{Mn} =\bar{V}_{Mn} \left( \underline{V}_{Mn}^{-1}\underline{\alpha}_{Mn}+ (\Sigma^{-1} \otimes X')y \right)
$$

```{r REG minnesota alpha, class.source = 'fold-show'}
alpha_Mn_post <- V_Mn_post %*% (solve(V_Mn) %*% alpha_Mn + kronecker(solve(Omega_Mn), t(X)) %*% y)
```


で与えられる。

**Minnesota prior は、OLS 推定量を事前平均の方向へ縮小する役割を果たし、  特に高次ラグおよび他変数のラグに対して強い縮小を課すことで、高次元 VAR における過剰パラメータ化を防ぐ。**

```{r EXP minnesota result, class.source = 'fold-show'}
print("推計結果（OLS v.s. Minnesota Prior")
tibble(OLS = as.vector(alpha_OLS), `Minn. Prior` = as.vector(alpha_Mn), Posterior = as.vector(alpha_Mn_post)) %>% round(2)
```

```{r EXP minnesota shrinkage}
A_OLS
A_Mn_post <- matrix(as.vector(alpha_Mn_post), nrow = K, ncol = M, byrow = F)

eq_names <- colnames(A_OLS)
if (is.null(eq_names)) eq_names <- paste0("eq", 1:M)

# ---- 1) OLSとPosteriorを縦持ちにする ----
df_ols <- as.data.frame(A_OLS) |>
  mutate(term = rownames(A_OLS)) |>
  pivot_longer(-term, names_to = "eq", values_to = "coef") |>
  mutate(method = "OLS")

df_post <- as.data.frame(A_Mn_post)
colnames(df_post) <- eq_names
df_post <- df_post |>
  mutate(term = rownames(A_OLS)) |>
  pivot_longer(-term, names_to = "eq", values_to = "coef") |>
  mutate(method = "Posterior")

df <- bind_rows(df_ols, df_post) |>
  # 定数は除外（ラグ軸に載らないので）
  filter(term != "const") |>
  # term から「説明変数」と「ラグ」を抽出（例: lRGDP.l3 -> var=lRGDP, lag=3）
  mutate(
    var = str_replace(term, "\\.l\\d+$", ""),
    lag = as.integer(str_extract(term, "(?<=\\.l)\\d+"))
  )

# ---- 2) 3変数×3方程式（=9パネル）に絞る（必要なら）----
# ここで var の順序を指定して見やすくする
var_levels <- c("lRGDP", "lDEF", "FF")
df <- df %>%
  mutate(
    var = factor(var, levels = var_levels),
    method = factor(method, levels = c("OLS", "Posterior"))
  )

# 方程式ごとにプロットを作って表示（3パネル：説明変数）
plots <- lapply(levels(factor(df$eq)), function(eq_i) {

  dfi <- df %>% filter(eq == eq_i)

  ggplot(dfi, aes(x = lag, y = coef, linetype = method, shape = method, group = method)) +
    geom_hline(yintercept = 0) +
    geom_line() +
    geom_point(size = 1.6) +
    scale_x_continuous(breaks = sort(unique(dfi$lag))) +
    facet_wrap(~ var, nrow = 3, scales = "free_y") +
    labs(
      x = "Lag",
      y = "Coefficient",
      linetype = "",
      shape = "",
      title = paste0("Equation: ", eq_i, " — OLS vs Minnesota Posterior"),
      subtitle = "Panels (free y-scale): regressors lRGDP, lDEF, FF"
    ) +
    theme_bw()
})

# すべて表示
for (g in plots) print(g)
```

### ミネソタ事前分布（ダミー観測値）

#### マッピング

Kilian and Lütkepohl (2016) のとおり、係数ベクトル  

$$
\alpha = \mathrm{vec}(A)
$$

に対する正規事前分布は、次の **dummy observation（mixed estimation）形式**で表現できる：

$$
\underbrace{C}_{KM\times KM}\alpha = \underbrace{c}_{KM\times 1} + e, \qquad e \sim N(0, I) \tag{5.2.8}
$$

ここで、

- $C$ は事前分散行列の精度に対応する行列
- $c$ は事前平均に対応するベクトル

であり、事前分布$\alpha \sim N(\underline{\alpha}_{Mn}, \underline{V}_{Mn})$と次の対応関係を持つ：

$$
\underline{V}_{Mn} = (C'C)^{-1}, \qquad \underline{\alpha}_{Mn} = C^{-1}c.
$$

すなわち、$C = \underline{V}_{Mn}^{-1/2}$、$c = C\underline{\alpha}_{Mn}$ とおけば、
dummy observation 表現と正規事前分布は完全に同値である。

一方、VAR($p$) モデルのベクトル化は、

$$
y = \underbrace{(I_M \otimes X)}_{TM\times KM} \alpha + \varepsilon, \qquad \varepsilon \sim N(0,\; \underbrace{\Sigma}_{M\times M}\otimes I_T) \tag{5.2.9}
$$

```{r REG minn dummy mapping, class.source = 'fold-show'}
Cinv <- chol(V_Mn)
# round(Cinv %*% (t(Cinv)),4)==round(V_Mn,4)
C <- solve(Cinv)
c <- C %*% alpha_Mn

y_dum <- rbind(matrix(y, ncol = 1), c)
Xbig_dum <- rbind(Xbig, C)

n_data  <- length(y)     # = TT*M
n_prior <- nrow(C)       # = K*M

Omega_data_inv  <- kronecker(solve(Omega_Mn), diag(TT))
Omega_prior_inv <- diag(n_prior)

Omega_inv <- matrix(0, n_data + n_prior, n_data + n_prior)
Omega_inv[1:n_data, 1:n_data] <- Omega_data_inv
Omega_inv[(n_data+1):(n_data+n_prior), (n_data+1):(n_data+n_prior)] <- Omega_prior_inv
```

となる（再掲）。

#### 拡張回帰

prior と likelihood を結合すると、次の **拡張回帰モデル**が得られる：

$$
\underbrace{\begin{bmatrix}y \\ c\end{bmatrix}}_{(TM+KM)\times 1}= \underbrace{ \begin{bmatrix} I_M \otimes X \\ C \end{bmatrix} }_{(TM+KM)\times KM} \alpha + \underbrace{ \begin{bmatrix} \varepsilon \\ e \end{bmatrix} }_{(TM+KM)\times 1}, 
$$

$$
\begin{bmatrix}
\varepsilon \\
e
\end{bmatrix}
\sim
N\!\left(
0,\;
\underbrace{
\begin{bmatrix}
\Sigma \otimes I_T & 0 \\
0 & I
\end{bmatrix}
}_{\Omega}
\right).
\tag{5.2.11}
$$

このモデルは、**prior を dummy observation として加えた GLS 問題**である。したがって、係数ベクトル $\alpha$ の **事後平均（MAP 推定量）**は、  
次の GLS 推定量として与えられる：

$$
\bar{\alpha}_{Mn}
=
\left(
\underbrace{
C'C
}_{\underline{V}_{Mn}^{-1}}
+
\underbrace{
(\Sigma^{-1}\otimes X'X)
}_{\text{likelihood precision}}
\right)^{-1}
\left(
\underbrace{
C'c
}_{\underline{V}_{Mn}^{-1}\underline{\alpha}_{Mn}}
+
\underbrace{
(\Sigma^{-1}\otimes X')
}_{}
y
\right).
$$

```{r REG minn dummy, class.source = 'fold-show'}
# ---- GLS estimator on augmented system ----
# alpha_gls = (X' Ω^{-1} X)^{-1} X' Ω^{-1} y
Xt_Oinv <- t(Xbig_dum) %*% Omega_inv
alpha_Mn_gls_dummy <- solve(Xt_Oinv %*% Xbig_dum, Xt_Oinv %*% y_dum)

# ---- compare with closed-form posterior mean ----
diff_vec <- as.vector(alpha_Mn_post - alpha_Mn_gls_dummy)
cat("Max abs diff (closed-form vs GLS dummy): ", max(abs(diff_vec)), "\n")
cat("Mean abs diff: ", mean(abs(diff_vec)), "\n")

# ---- optional: reshape to K x M for readability ----
A_Mn_gls_dummy <- matrix(as.vector(alpha_Mn_gls_dummy), nrow = K, ncol = M, byrow = FALSE)
colnames(A_Mn_gls_dummy) <- colnames(A_OLS)
rownames(A_Mn_gls_dummy) <- rownames(A_OLS)
```

これは、正規事前分布と正規尤度の共役性から得られるclosed-form の事後平均

$$
\bar{\alpha}_{Mn}
=
\bar{V}_{Mn}
\left(
\underline{V}_{Mn}^{-1}\underline{\alpha}_{Mn}
+
(\Sigma^{-1}\otimes X')y
\right),
\qquad
\bar{V}_{Mn}
=
\left(
\underline{V}_{Mn}^{-1}
+
\Sigma^{-1}\otimes X'X
\right)^{-1}
$$

と完全に一致する。

このように、Minnesota prior を用いた Bayesian VAR は、

> **「本来のデータに、prior に由来する dummy observation を加えた GLS 推定」**

として解釈できる。


#### Koop & Korobilis (2010) と Kilian & Lütkepohl (2016) の notation の違い

Bayesian VAR のベクトル化表現は、Koop and Korobilis (2010, KK) と  
Kilian and Lütkepohl (2016, KL) で見かけ上異なる形で書かれているが、  
**適切な並べ替えを行えば数学的に完全に同値**である。

以下では、その違いと注意点を整理する。

---

##### 1. ベクトル化（vec）の取り方

- **KK**  
  被説明変数行列 $Y$ に対して **列方向の vec** を取る：
  $$
  y = \mathrm{vec}(Y)
  =
  \begin{bmatrix}
  Y_{:,1} \\
  Y_{:,2} \\
  \vdots \\
  Y_{:,M}
  \end{bmatrix}
  $$
  （各方程式ごとに時間方向にスタック）

- **KL**  
  ブロック回帰の観点から、
  **時間方向を先に並べた表現**を用いる。

この違いが、設計行列や誤差分散の Kronecker 積の順序の差として現れる。

---

##### 2. 設計行列の違い

VAR($p$) を
$$
Y = XA + E
$$
と書くとき、

- **KK の表現**
  $$
  y
  =
  \underbrace{(I_M \otimes X)}_{\text{KK}}
  \alpha
  +
  e,
  \qquad
  \alpha = \mathrm{vec}(A)
  $$

- **KL の表現**
  $$
  y
  =
  \underbrace{(X \otimes I_M)}_{\text{KL}}
  \tilde{\alpha}
  +
  u
  $$

両者は、係数ベクトルの並び替え
$$
\tilde{\alpha} = P \alpha
$$
を用いれば同一の線形写像を表している。

---

##### 3. 誤差分散行列の違い

- **KK**
  $$
  e \sim N(0,\; \Sigma \otimes I_T)
  $$

- **KL**
  $$
  u \sim N(0,\; I_T \otimes \Sigma)
  $$

これは誤差項の分布が異なるのではなく、  
**ベクトルの並び順の違いによる見かけの差**である。

行列正規分布
$$
E \sim MN(0,\; I_T,\; \Sigma)
$$
に対して、どの順序で vec を取るかによって  
Kronecker 積の順序が入れ替わる。

---

##### 4. 本質的に同値である理由

Kronecker 積と vec の基本恒等式により、
$$
(I_M \otimes X)\,\mathrm{vec}(A)
\quad \text{と} \quad
(X \otimes I_M)\,P\,\mathrm{vec}(A)
$$
は、適切な置換行列 $P$ の下で同一である。

したがって、KK と KL の違いは

> **モデルの違いではなく、表現（notation）の違い**

に過ぎない。

---

##### 5. 実装上の重要な注意点

- 一度 **KK 流**（$I_M \otimes X$, $\Sigma \otimes I_T$）を採用したら、
  - 係数ベクトルの並び
  - dummy observation
  - GLS / whitening
  を **すべてその規約で統一**する必要がある。

- KL の式をそのままコードに写すと、
  - Kronecker 積の順序
  - 誤差分散の扱い
  が一致せず、誤った重み付けになる危険がある。

---

### 6. まとめ

- Koop and Korobilis (2010) と Kilian and Lütkepohl (2016) の VAR 表現は  
  **完全に同値**である。
- 違いは
  - vec の定義
  - 係数ベクトルの並び
  - Kronecker 積の順序
  に起因する **記法上の差**である。
- 実装では、**どちらの規約を使うかを最初に決め、最後まで貫くことが必須**である。





https://www.r-econometrics.com/timeseries/bvar/   

T: 73 samples: 76 terms (1960Q1-1978Q4) - 2 lags - 1 dlog
p: 2 lags
k: 3 vars (invest, income, cons)


<!-- ```{r setup} -->
<!-- library(bvartools) -->
<!-- data("e1") -->
<!-- e1 <- diff(log(e1)) # Calculate first-log-differences -->
<!-- e1 <- window(e1, end = c(1978, 4)) # Reduce nobs -->
<!-- e1 <- e1 * 100 # Rescale data -->
<!-- plot(e1) # Plot the series -->
<!-- data <- gen_var(e1, p = 2, deterministic = "const") -->
<!-- ``` -->


<!-- $$ -->
<!-- \text{Let }Y = ( 3 \times 73)\text{, } X = (7 \times 73).\\ -->
<!-- A_{freq} = YX^{'}(XX^{'})^{-1}. -->
<!-- $$ -->

<!-- $$ -->
<!-- \text{Let }Y^* = ( 1 \times 219)\text{, } X^* = I_M \otimes X.\\ -->
<!-- vec(A_{freq}) = vec(Y){vec(X)}^{'}[vec(X){vec(X)}^{'}]^{-1}, where\\ -->
<!-- $$ -->

<!-- ```{r ols} -->
<!-- ### A -->
<!-- y <- t(data$data$Y) # K * T = 3 * 73 -->
<!-- x <- t(data$data$Z) # (K*p)+1 * T = 7 * 73 -->
<!-- A_freq <- tcrossprod(y, x) %*% solve(tcrossprod(x)) # K * (K*p)+1 = 3 * 7 -->
<!-- A_freq_mine <-  solve(x %*% t(x)) %*% (x %*% t(y)) # K * (K*p)+1 = 3 * 7 -->

<!-- ### A (vec expression) chatGPT -->
<!-- my_vec <- function(mat){ret <- matrix(mat)} -->
<!-- y_vec <- t(my_vec(y)) #  1 * (T*K) = 1 * 219 -->
<!-- x_vec <- kronecker(x, diag(1,3)) # {K * K} kron {(K*p)+1 * T} = 219 * 21 -->
<!-- A_freq_vec <- (y_vec %*% t(x_vec))  %*% solve(x_vec %*% t(x_vec)) # 1 * 21 -->
<!-- # y_vec_mine <- my_vec(t(y)) # (T*K) * 1 = 219 * 1 -->
<!-- # x_vec_mine <- kronecker(diag(1,3),t(x)) # {K * K} kron {(K*p)+1 * T} = 21 * 219 -->
<!-- # A_freq_vec_mine <- solve(t(x_vec) %*% x_vec) %*% (t(x_vec) %*% y_vec) # 21 * 1 -->


<!-- ### U -->
<!-- u_freq <- y - A_freq %*% x # K * T = 3 * 73 -->
<!-- u_sigma_freq <- tcrossprod(u_freq) / (ncol(y) - nrow(x)) # K * K = 3 * 3 -->

<!-- # Reset random number generator for reproducibility -->
<!-- set.seed(1234567) -->

<!-- iter <- 30000 # Number of iterations of the Gibbs sampler -->
<!-- burnin <- 15000 # Number of burn-in draws -->
<!-- store <- iter - burnin -->

<!-- tt <- ncol(y) # Number of observations -->
<!-- k <- nrow(y) # Number of endogenous variables -->
<!-- m <- k * nrow(x) # Number of estimated coefficients -->
<!-- ``` -->

<!-- ```{r bayesian setup} -->
<!-- # Reset random number generator for reproducibility -->
<!-- set.seed(1234567) -->

<!-- iter <- 30000 # Number of iterations of the Gibbs sampler -->
<!-- burnin <- 15000 # Number of burn-in draws -->
<!-- store <- iter - burnin -->

<!-- tt <- ncol(y) # Number of observations -->
<!-- k <- nrow(y) # Number of endogenous variables -->
<!-- m <- k * nrow(x) # Number of estimated coefficients -->
<!-- ``` -->

<!-- ```{r bayesian prior} -->
<!-- a_mu_prior <- matrix(0, m) # Vector of prior parameter means -->
<!-- a_v_i_prior <- diag(1, m) # Inverse of the prior covariance matrix -->

<!-- u_sigma_df_prior <- 6 # Prior degrees of freedom -->
<!-- u_sigma_scale_prior <- diag(1, k) # Prior covariance matrix -->
<!-- u_sigma_df_post <- tt + u_sigma_df_prior # Posterior degrees of freedom -->

<!-- ``` -->

<!-- ```{r Gibbs} -->
<!-- # Data containers for posterior draws -->
<!-- # str_lab <- expand.grid(v2 = colnames(A_freq), v1 = rownames(A_freq)) %>%  -->
<!-- #   with(., paste(v1, v2, sep = "_")) -->
<!-- str_y <- rownames(A_freq) -->
<!-- str_x <- colnames(A_freq) -->
<!-- str_lab <- outer(str_y, str_x, paste, sep = "_") %>%  -->
<!--   as.vector() -->
<!-- draws_a <- matrix(NA, m, store) %>%  -->
<!--     `rownames<-`(str_lab) -->
<!-- draws_a_mine <- matrix(NA, m, store) %>%  -->
<!--   `rownames<-`(str_lab) -->

<!-- draws_sigma <- matrix(NA, k * k, store) -->

<!-- # Initial values -->
<!-- u_sigma_i <- solve(u_sigma_freq) -->

<!-- # Start Gibbs sampler -->
<!-- for (draw in 1:iter) { -->
<!--   # Draw conditional mean parameters -->
<!--   a <- post_normal(y, x, u_sigma_i, a_mu_prior, a_v_i_prior) -->
<!--   ### Mumtaz p.34 matlab code -->
<!--   v_mine <- solve(a_v_i_prior + kronecker(u_sigma_i,x %*% t(x))) -->
<!--   m_mine <- v_mine %*% (a_v_i_prior %*% a_mu_prior + kronecker(u_sigma_i,x %*% t(x)) %*% t(A_freq_vec)) -->
<!--   a_mine <- MASS::mvrnorm(1, m_mine, v_mine) %>% as.matrix() -->

<!--   # Draw variance-covariance matrix -->
<!--   u <- y - matrix(a, k) %*% x # Obtain residuals -->
<!--   u_sigma_scale_post <- solve(u_sigma_scale_prior + tcrossprod(u)) -->
<!--   u_sigma_i <- matrix(rWishart(1, u_sigma_df_post, u_sigma_scale_post)[,, 1], k) -->
<!--   u_sigma <- solve(u_sigma_i) # Invert Sigma_i to obtain Sigma -->

<!--   # Store draws -->
<!--   if (draw > burnin) { -->
<!--     draws_a[, draw - burnin] <- a -->
<!--     draws_a_mine[, draw - burnin] <- a_mine -->
<!--     draws_sigma[, draw - burnin] <- u_sigma -->
<!--   } -->
<!-- } -->

<!-- ``` -->


<!-- ```{r Gibbs} -->
<!-- ### outputs -->

<!-- tbl_A_bayes <- rbind( -->
<!--   mutate(t(draws_a) %>% as_tibble(), algo = "package"), -->
<!--   mutate(t(draws_a_mine) %>% as_tibble(), algo = "mine") -->
<!-- ) -->

<!-- tbl_A_freq <- tibble(name = str_lab, value = my_vec(A_freq) %>% as.numeric()) %>%  -->
<!--   transmute(y = str_remove(name, "_.*$") %>% factor(levels = str_y),  -->
<!--             x = str_remove(name, "^.*_") %>% factor(levels = str_x), -->
<!--             value) -->

<!-- tbl_A_bayes_dens <- tbl_A_bayes %>%  -->
<!--   pivot_longer(cols = -algo) %>%  -->
<!--   transmute(algo,  -->
<!--             y = str_remove(name, "_.*$") %>% factor(levels = str_y),  -->
<!--             x = str_remove(name, "^.*_") %>% factor(levels = str_x), -->
<!--             value) -->

<!-- tbl_A_bayes_ci <- tbl_A_bayes_dens %>%  -->
<!--   group_by(algo, y, x) %>%  -->
<!--   summarize(ltile = quantile(value, 0.05), -->
<!--             median = quantile(value, 0.50), -->
<!--             utile = quantile(value, 0.95)) %>%  -->
<!--   ungroup() -->


<!-- ggplot() + -->
<!--   geom_density(data = tbl_A_bayes_dens, -->
<!--                mapping = aes(x = value, color = algo)) + -->
<!--   facet_wrap(~ x + y, ncol = 3,scales = "free")+ -->
<!--   # geom_point(data = tbl_A_freq, mapping = aes(x = value, y = 0))+ -->
<!--   geom_vline(data = tbl_A_freq, aes(xintercept = value))+ -->
<!--   geom_point(data = tbl_A_bayes_ci %>% filter(algo == "package"), -->
<!--              mapping = aes(x = ltile, y = 0))+ -->
<!--   geom_point(data = tbl_A_bayes_ci %>% filter(algo == "package"), -->
<!--              mapping = aes(x = utile, y = 0))+ -->
<!--   geom_point(data = tbl_A_bayes_ci %>% filter(algo == "package"), -->
<!--              mapping = aes(x = median, y = 0)) -->


<!-- ``` -->


